\chapter{Problem Formulation}\label{ch:problem_formulation}

This chapter systematically formulates the targetted task of robotic grasping as an MDP, while describing the applied reward function alongside the utilised observation and action spaces. The corresponding implementation of such formulation with detailed specifications is covered in the next chapter \autoref{ch:implementation}.

In this work, the agent is assumed to be a high-level controller that provides sequential decision making in form of gripper poses and actions. Therefore, the environment is considered to not only include all objects and the physical interactions between them but also the robot with its actuators and low-level controllers. Episodic formulation of the grasping task is studied in this work, where a new set of objects is introduced into the scene at the beginning of each episode when the environment is reset. During each episode, the aim of the agent is to grasp and lift an object certain height above the ground plane that the objects rest on, which also terminates the current episode. Furthermore, an episode is also terminated after a fixed number of time steps and whenever the agent pushes all objects outside the union of the perceived and reachable workspace. Placing of objects after their picking is not investigated in this work.

Due to the benefits of employing robotics simulators to train RL agents, e.g. safe and inexpensive data collection, robotics simulator will be used in this work to train the agent. Once the agent is trained in a virtual environment, the learned policy will subsequently be evaluated in a real-world setup via sim-to-real transfer. The conceptual setup of this work that should be similar in both domains is illustrated in \autoref{fig:problem_formulation_setup_sketch}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.667\textwidth]{problem_formulation/setup_sketch.pdf}
    \caption{Conceptual setup for the task of robotic grasping that needs to be constructed both inside a robotics simulator for training and in real-world domain for subsequent evaluation. The goal of the RL agent is to grasp and lift any object from the scene that is perceived by a statically mounted RGB-D camera.}
    \label{fig:problem_formulation_setup_sketch}
\end{figure}


\section{Observation Space}

The observation space for the grasping task used in this work comprises of visual and proprioceptive observations. Furthermore, a number of sequential observations is stacked together for each transition in order to provide the agent with temporal information about environment states.


\subsection{Octree}

The visual observations utilised in this work are represented in form of 3D octrees. As already mentioned, the visual perception originates from a statically mounted RGB-D camera, which is assumed to provide a new RGB image and depth map of the scene at each time step. Before constructing an octree, the depth map is first used to create a point cloud of the scene as an intermediate representation. This point cloud is colourised with a corresponding RGB image that is registered to the optical frame of the camera's depth sensor. Therefore, the resulting point cloud is in form of an unstructured list of~\((x,y,z,r,g,b)\) tuples that represent individual points.

Hereafter, three assumptions about the use of volumetric 3D data representation for end-to-end robotic manipulation are set forth. First, aspect ratio of~1:1:1 is considered to provide generalisation over all possible directions of movement, i.e.~traversing a fixed distance along any of the primary axes should result in a movement over the same number of cells. Second assumption considers the volume that each cell occupies, which shall remain fixed over the entire duration of training and evaluation. This is considered to be beneficial because fixed scale of cells provides a consistency over distances between any two cells. Lastly, each cell should correspond to a specific position of space that remains fixed with respect to the robot pose, regardless of the camera pose. This assumption is considered to be necessary as it allows NNs to create relations among individual cells and their significance in space.

Due on these assumptions, the approach that is commonly used in classification and segmentation tasks, i.e.~rescale a point cloud to fit inside a fixed volume \cite{wang_o-cnn_2017}, cannot be applied in this work. However, it is assumed that the relative pose of camera with respect to robot is known, e.g.~through calibration process, therefore, the previously obtained point cloud is transformed into the robot coordinate frame in order to achieve invariance to camera pose. Furthermore, such point cloud is subsequently cropped in order to occupy a fixed volume in space with aspect ratio of~1:1:1. This volume is considered to be the observed workspace and it is subsequently used to construct the octree observations as illustrated in \autoref{fig:problem_formulation_octree_creation_sketch}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{problem_formulation/octree_creation_sketch.pdf}
    \caption{Process of constructing an octree from depth map and RGB image via an intermediate point cloud, which is transformed into the robot coordinate frame and cropped to a fixed volume.}
    \label{fig:problem_formulation_octree_creation_sketch}
\end{figure}

The octree structure by \citet{wang_o-cnn_2017} allows arbitrary data to be stored at the finest leaf octants. Three distinct features are utilised in this work, namely the average unit normal vector~\(\overline{n}\), the average distance between the centre of the cell and points that formed it~\(\overline{d}\) and the average colour~\(\overline{rgb}\). As illustrated in \autoref{fig:problem_formulation_octree_features}, all of these features are computed independently for each octant based on the points from the point cloud that produced it. Normals~\(n_{i}~{=}~(n_{x_{i}},n_{y_{i}},n_{z_{i}})\) are selected because they provide smoothness-preserving description of the object surfaces, as previously shown in \autoref{fig:rw_ocnn_occupancy_vs_normals}. Since point cloud acquired from RGB-D camera does not usually contain normals, they must be estimated from a local neighbourhood prior to constructing an octree. The average distance to the points~\(\overline{d}\) allows the perceived surface to be offset in the direction of normals, which allows octrees with lower resolution to be used while still preserving smooth transitions between the cells. Colour features~\(rgb_{i}~{=}~(r_{i},g_{i},b_{i})\) are expected to provide an agent with additional input that could allow semantic analysis in addition to shape analysis, which might be especially beneficial for distinguishing dissimilar objects that are in contact. Besides~\(\overline{n}\) being normalised as a unit vector,~\(\overline{d}\) and all channels of~\(\overline{rgb}\) are normalised to be in a range \([0,1]\).

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{problem_formulation/octree_features_sketch.pdf}
    \caption{Representation of features for a single finest leaf octant from an octree. All points from the source point cloud are used to determine the average unit normal vector~\(\overline{n}\), average distance to the centre of the cell~\(\overline{d}\) and the average colour~\(\overline{rgb}\).}
    \label{fig:problem_formulation_octree_features}
\end{figure}


\subsection{Proprioceptive Observations}

In addition to the visual observations acquired by an RGB-D camera, it is considered to be beneficial to also include proprioceptive observations. Gripper pose and gripper state are used in this work because these observations are independent of the utilised robot. Although both of these could be determined solely from the visual observations, occlusion can introduce significant uncertainness. Furthermore, these readings are easily obtainable from any robot. State of the gripper is represented simply as~\(\{opened: 1, closed: -1\}\). The position of the gripper is encoded as~\((x,y,z)\) vector represented with respect to robot's base frame. Gripper orientation is also with respect to robot's base frame, and represented as the first two columns of the rotation matrix~\([(R_{11},R_{21},R_{31}),(R_{21},R_{22},R_{23})]\) because they provide continuous description of 3D orientation without ambiguities, contrary to quaternions of Euler angles \cite{zhou_continuity_2020}.


\subsection{Observation Stacking}

A single set of visual and proprioceptive observations does not fully describe the state of the environment. In order to better satisfy Markov assumption, dynamics of the system must also be observed, including all data based on the temporal information. \citet{mnih_human-level_2015} addressed this in a simple way by stacking last~\(n\) historical observations together and combining them into a single observation that fully describes the state. Despite the increase in the amount of data that needs to be processed, this work applies a similar observation stacking method due to the simplicity of such solution.


\section{Action Space}

% Mention this here
% Some problems are better solved with other traditional method instead of RL. (when talking about control of joint vs cartesian pose)


\section{Reward Function}



\section{Curriculum and Demonstrations}


