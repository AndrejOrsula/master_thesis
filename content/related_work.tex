\chapter{Related Work}

Robotic manipulation and grasping is a field that has been extensively studied for decades via magnitude of different approaches. This chapter outlines some of the notable methods, while focusing on contributions that employ model-free reinforcement learning due to their relevance for this project.


\section{Analytical Approaches}

Analytical approaches determine grasps that satisfy target requirements through kinematic and dynamic formulations \cite{sahbani_overview_2012}. These methods typically analyze the geometry of target object and utilised gripper in order to generate suitable grasp pose, which can then be reached by using a separate motion planner. The approach was introduced by \citet{nguyen_constructing_1987} through formulation of objectives for constructing stable force-closure grasps on polyhedral objects. By modelling objects as triangular mesh or 3D point cloud, force-closure grasps were later extended to remove model restrictions \cite{yun-hui_liu_complete_2004}. Several analytical metrics for estimating the quality of grasps were also introduced over the years to quantify good grasps \cite{roa_grasp_2015}, many of which have found their applicability beyond analytical approaches.

Expert human knowledge of robot in specific task is required and can be used directly to develop these algorithm, which allows it them achieve very efficient operation on a number of selected objects. However, this also introduces a limitation because performance is restricted only to the predicted situations and scalable generalisation to novel objects is often unfeasible due to computation complexity that arises from the number of considered conditions \cite{sahbani_overview_2012}. Moreover, geometric models of objects might not be available before interaction is required. Partial occlusion of objects in setups with passive perception similarly limits the use of geometrical analysis.


\section{Supervised Learning}

Empirical methods were introduced to overcome shortcomings and difficulties of analytical approaches by combining sampling and training to achieve learning, which in turn reduces or removes the need to manually develop a model. A common approach is to use supervised learning to detect grasp poses by training on a dataset that is labelled with binary flags that indicate whether a region contains grasps or not. Alternatively, a combination of analytical grasp quality metrics can be used to provide a more fine-tuned labelling of data \cite{roa_grasp_2015}. In this way, \citet{saxena_robotic_2008} applied supervised learning for detection of grasps on previously unseen objects by using handcrafted features from two or more RGB images of the scene in order to identify points in each image that correspond to grasp locations. They then determine the 3D position of detected grasps via triangulation and use custom heuristics to estimate orientation before planning a collision-free path.

Due to the significant advancements of deep learning (DL) in recent years, there has been a trend towards applying DL for robotic grasping. \citet{lenz_deep_2015} developed a framework that used DL to train two separate neural networks (NNs) on RGB-D data, where a small network was used to search for image patches with potential grasp candidates, and a larger network then ranked these candidates to select the most optimal grasp. With this work, \citeauthor{lenz_deep_2015} demonstrated the advantage of using DL instead of time-consuming design of hand-crafted features for robotic grasping. Popularity of convolution neural networks (CNNs) in computer vision applications also inspired their use for robotic grasping, which resulted in more accurate and computationally efficient systems for predicting grasps in RGB-D images \cite{redmon_real-time_2015, kumra_robotic_2017}.

Besides 2D images, supervised DL methods have also been applied to 3D point clouds. Approach by \citet{ten_pas_grasp_2017} randomly samples a large number of grasp candidates uniformly from the object surface using a point cloud, without a need to segment the individual objects first. They subsequently encode a region of interest around each grasp candidate as a stacked multi-channel projected image, which is then scored by the use of CNN classifier. By selecting the grasp candidate with the highest score, they were able to demonstrate a success rate of 93\% on novel objects in dense clutter.


