\chapter{Related Work}

Robotic manipulation and grasping is a field that has been extensively studied for decades via magnitude of different approaches. This chapter outlines some of the notable methods, while mainly focusing on contributions that employ model-free reinforcement learning due to their relevance for this project.


\section{Analytical Approaches}

Analytical approaches determine grasps that satisfy target requirements through kinematic and dynamic formulations \cite{sahbani_overview_2012}. These methods typically analyze the geometry of target object and utilised gripper in order to generate a suitable grasp pose, which can then be reached by using a separate motion planner. The approach was introduced by \citet{nguyen_constructing_1987} through formulation of objectives for constructing stable force-closure grasps on polyhedral objects. By modelling objects as triangular mesh or 3D point cloud, force-closure grasps were later extended to remove model restrictions \cite{yun-hui_liu_complete_2004}. Several analytical metrics for estimating the quality of grasps were also introduced over the years to quantify good grasps \cite{roa_grasp_2015}, many of which have found their applicability beyond analytical approaches.

Expert human knowledge of robot in a specific task is required to develop these algorithm, which allows them to achieve very efficient operation on a number of selected objects due to direct transfer of this knowledge. However, this also introduces a limitation because performance is restricted only to the predicted situations and scalable generalisation to novel objects is often unfeasible due to computation complexity that arises from the number of considered conditions \cite{sahbani_overview_2012}. Moreover, geometric models of objects might not be available before interaction is required, and partial occlusion of objects in setups with passive perception similarly limits the use of geometrical analysis.


\section{Supervised Learning}

Empirical methods were introduced to overcome shortcomings and difficulties of analytical approaches by combining sampling and training to achieve learning, which in turn reduces or removes the need to manually develop a model. A common approach is to use supervised learning to detect grasp poses by training on a dataset that is labelled with indication about what regions contain grasps \cite{saxena_robotic_2008, lenz_deep_2015}. Alternatively, a combination of analytical grasp quality metrics can be used to provide a more fine-tuned labelling of data \cite{mahler_dex-net_2017, mahler_dex-net_2018, mahler_learning_2019, lundell_robust_2019}. \citet{saxena_robotic_2008} applied supervised learning for detection of grasps on previously unseen objects by using handcrafted features from two or more RGB images of the scene in order to identify points in each image that correspond to grasp locations. They then determine the 3D position of detected grasps via triangulation and use custom heuristics to estimate orientation before planning a collision-free path.

Due to the significant advancements of deep learning (DL) in recent years, there has been a trend towards applying DL for robotic grasping. \citet{lenz_deep_2015} developed a framework that used DL to train two separate neural networks (NNs) on RGB-D data, where a small network was used to search for image patches with potential grasp candidates, and a larger network then ranked these candidates to select the most optimal grasp. With this work, \citeauthor{lenz_deep_2015} demonstrated the advantage of using DL instead of time-consuming design of hand-crafted features for robotic grasping. Popularity of convolution neural networks (CNNs) in computer vision applications also inspired their use for robotic grasping, which resulted in more accurate systems for predicting grasps in RGB-D images \cite{redmon_real-time_2015, kumra_robotic_2017}. The use of CNN also provides computationally efficiency, which allowed \citet{morrison_closing_2018} to synthesize grasps from depth images in real-time and perform closed-loop control.

Besides 2D images, supervised DL methods have also been applied to 3D data representations. Approach by \citet{ten_pas_grasp_2017} randomly samples a large number of grasp candidates uniformly from the object surface using a point cloud, without a need to segment the individual objects first. They subsequently encode a region of interest around each grasp candidate as a stacked multi-channel projected image, which is then scored by the use of CNN classifier. By selecting the grasp candidate with the highest score, they were able to demonstrate a success rate of 93\% on novel objects in a dense clutter. \citet{lundell_robust_2019} used DL on voxel grid for shape completion of partially observed objects in order to obtain multiple predictions of the full object shape. These predictions were then used to jointly evaluate analytical grasp metrics \cite{roa_grasp_2015} for all grasp candidates, which they sampled from a mesh constructed as mean of all shape predictions. With this work, \citeauthor{lundell_robust_2019} demonstrated improved success rate over methods using a partial view or a single shape estimate.

Although supervised learning approaches can achieve high success rate, their main disadvantage is the large volume of labelled data required to effectively learn grasp generation. It is non-trivial to perform manual labelling on grasping data because objects can be grasped in multiple ways, furthermore, human labeling introduces bias \cite{pinto_supersizing_2015}. To avoid this time-consuming process, majority of recent work relies on synthetically generated datasets. As an example, \citet{mahler_dex-net_2017} achieved 99\% precision by training on a dataset with 6.7 million point clouds of more than 10,000 unique 3D models, each containing grasps and corresponding analytical grasp metrics. Generalization to other gripper types is also limited and the entire dataset needs to be updated in order to support new types, which is why they created a new dataset of 2.8 million point clouds in \citeyear{mahler_dex-net_2018} for vacuum-based grippers. This issue was later addressed by creating a common dataset for both parallel-jaw and vacuum-based grippers by using a more general analytical metric based on object's expected resistance to forces and torques \cite{mahler_learning_2019}.


\section{Imitation Learning}

Another empirical method is based on the process of learning tasks from demonstrations, called imitation learning. Demonstrations are normally represented as trajectories containing states or state-action pairs, which can be obtained in several different ways such as teleoperation, kinesthetic teaching, or motion capture \cite{osa_algorithmic_2018}. In this way, imitation learning aims to provide robots with a desired behaviour by simply showing a sequence of actions instead of manually programming them. 

Behavioural cloning is the simplest form of imitation learning, in which a policy that directly maps states to actions is learned through techniques such as non-linear regression or support vector machines \cite{osa_algorithmic_2018}. Recently, \cite{zhang_deep_2018} showed that DL allows behavioural cloning to be an effective way for robots to acquire complex skills. They used a virtual reality headset and hand-tracking controller to acquire teleoperated demonstrations in the form of RGB-D images, which were subsequently used to train a deep policy by the use of CNN. With this approach, \citeauthor{zhang_deep_2018} managed to train a simple grasping task with one object to 97\% success rate while using 180 distinct demonstrations. An emerging category termed Learning from Observation (LfO) aims to learn policy in a similar way from visual demonstrations that do not have any labels associated with them and the state might not be fully known \cite{kroemer_review_2021}.

Even though the use of imitation learning provides a quick way of acquiring new policies, demonstrations usually do not contain all possible states that the robot might experience because collecting expert demonstrations for all scenarios can become too expensive and time-consuming \cite{osa_algorithmic_2018}. For this reason, the learned policy might struggle to generalize to novel objects and situations.

