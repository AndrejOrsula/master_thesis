
@book{sutton_reinforcement_2018,
	address = {Cambridge, MA, USA},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	publisher = {A Bradford Book},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	pages = {529--533},
	file = {Snapshot:/home/andrej/Zotero/storage/KZVHNVLP/nature14236.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Submitted Version:/home/andrej/Zotero/storage/6IMRYRPU/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@article{schrittwieser_mastering_2020,
	title = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
	volume = {588},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03051-4},
	doi = {10.1038/s41586-020-03051-4},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	language = {en},
	number = {7839},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = dec,
	year = {2020},
	note = {Number: 7839
Publisher: Nature Publishing Group},
	pages = {604--609},
	file = {Snapshot:/home/andrej/Zotero/storage/BE6MS6T2/s41586-020-03051-4.html:text/html;Submitted Version:/home/andrej/Zotero/storage/JATC7FIM/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf:application/pdf},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	pages = {350--354},
	file = {Snapshot:/home/andrej/Zotero/storage/5SDL7JC8/s41586-019-1724-z.html:text/html},
}

@article{sahbani_overview_2012,
	series = {Autonomous {Grasping}},
	title = {An overview of {3D} object grasp synthesis algorithms},
	volume = {60},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889011001485},
	doi = {10.1016/j.robot.2011.07.016},
	abstract = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger–object contact interactions Bicchi and Kumar (2000) [12] or robot hand design and their control Al-Gallaf et al. (1993) [70]. Robot grasp synthesis algorithms have been reviewed in Shimoga (1996) [71], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
	language = {en},
	number = {3},
	urldate = {2021-04-18},
	journal = {Robotics and Autonomous Systems},
	author = {Sahbani, A. and El-Khoury, S. and Bidaud, P.},
	month = mar,
	year = {2012},
	keywords = {Force-closure, Grasp synthesis, Learning by demonstration, Task modeling},
	pages = {326--336},
	file = {ScienceDirect Full Text PDF:/home/andrej/Zotero/storage/9QSV9H4L/Sahbani et al. - 2012 - An overview of 3D object grasp synthesis algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/andrej/Zotero/storage/62GHM7TM/S0921889011001485.html:text/html},
}

@article{kroemer_review_2021,
	title = {A {Review} of {Robot} {Learning} for {Manipulation}: {Challenges}, {Representations}, and {Algorithms}},
	shorttitle = {A {Review} of {Robot} {Learning} for {Manipulation}},
	abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
	journal = {J. Mach. Learn. Res.},
	author = {Kroemer, Oliver and Niekum, S. and Konidaris, G.},
	year = {2021},
	file = {Full Text PDF:/home/andrej/Zotero/storage/SFIZLJYN/Kroemer et al. - 2021 - A Review of Robot Learning for Manipulation Chall.pdf:application/pdf},
}

@article{yun-hui_liu_complete_2004,
	title = {A complete and efficient algorithm for searching 3-{D} form-closure grasps in the discrete domain},
	volume = {20},
	issn = {1941-0468},
	doi = {10.1109/TRO.2004.829500},
	abstract = {A complete and efficient algorithm is proposed for searching form-closure grasps of n hard fingers on the surface of a three-dimensional object represented by discrete points. Both frictional and frictionless cases are considered. This algorithm starts to search a form-closure grasp from a randomly selected grasp using an efficient local search procedure until encountering a local minimum. The local search procedure employs the powerful ray-shooting technique to search in the direction of reducing the distance between the convex hull corresponding to the grasp and the origin of the wrench space. When the distance reaches a local minimum in the local search procedure, the algorithm decomposes the problem into a few subproblems in subsets of the points according to the existence conditions of form-closure grasps. A search tree whose root represents the original problem is employed to perform the searching process. The subproblems are represented as children of the root node and the same procedure is recursively applied to the children. It is proved that the search tree generates O(KlnK/n) nodes in case a from-closure grasp exists, where K is the number of the local minimum points of the distance in the grasp space and n is the number of fingers. Compared to the exhaustive search, this algorithm is more efficient, and, compared to other heuristic algorithms, the proposed algorithm is complete in the discrete domain. The efficiency of this algorithm is demonstrated by numerical examples.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {{Yun-Hui Liu} and {Miu-Ling Lam} and Ding, D.},
	month = oct,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Automation, Computer science education, Councils, Educational programs, Educational technology, Fingers, Fixtures, Heuristic algorithms, Intelligent robots, Testing},
	pages = {805--816},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/B5VH23G6/Yun-Hui Liu et al. - 2004 - A complete and efficient algorithm for searching 3.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/BQRMWVGA/1339381.html:text/html},
}

@inproceedings{nguyen_constructing_1987,
	title = {Constructing stable grasps in {3D}},
	volume = {4},
	doi = {10.1109/ROBOT.1987.1088008},
	abstract = {This paper presents fast and simple algorithms for directly constructing stable grasps in 3D. The synthesis of stable grasps constructs virtual springs at the contacts, such that the grasped object is stable, and has a desired stiffness matrix about its stable equilibrium. The paper develops a simple geometric relation between the stiffness of the grasp and the spatial configuration of the virtual springs at the contacts. The stiffness of the grasp also depends on whether the points of contact stick, or slide without friction on the edges of the object.},
	booktitle = {1987 {IEEE} {International} {Conference} on {Robotics} and {Automation} {Proceedings}},
	author = {Nguyen, V.-},
	month = mar,
	year = {1987},
	keywords = {Fingers, Artificial intelligence, Friction, Paper technology, Research and development, Resists, Rubber, Springs, Stability, Tendons},
	pages = {234--239},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/6RUS2LUC/Nguyen - 1987 - Constructing stable grasps in 3D.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/A86EDLN3/1088008.html:text/html},
}

@article{roa_grasp_2015,
	title = {Grasp quality measures: review and performance},
	volume = {38},
	issn = {1573-7527},
	shorttitle = {Grasp quality measures},
	url = {https://doi.org/10.1007/s10514-014-9402-3},
	doi = {10.1007/s10514-014-9402-3},
	abstract = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
	language = {en},
	number = {1},
	urldate = {2021-04-19},
	journal = {Autonomous Robots},
	author = {Roa, Máximo A. and Suárez, Raúl},
	month = jan,
	year = {2015},
	pages = {65--88},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/7X5NQZ6W/Roa and Suárez - 2015 - Grasp quality measures review and performance.pdf:application/pdf},
}

@article{saxena_robotic_2008,
	title = {Robotic {Grasping} of {Novel} {Objects} using {Vision}},
	volume = {27},
	doi = {10.1177/0278364907087172},
	abstract = {We consider the problem of grasping novel objects, specifically ones that are being seen for the first time through vision. Grasping a previously un- known object, one for which a 3-d model is not available, is a challenging problem. Further, even if given a model, one still has to decide where to grasp the object. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Given two (or more) images of an ob- ject, our algorithm attempts to identify a few points in each image corresponding to good locations at which to grasp the object. This sparse set of points is then triangulated to obtain a 3-d location at which to attempt a grasp. This is in contrast to standard dense stereo, which tries to triangulate every single point in an image (and often fails to return a good 3-d model). Our algorithm for identifying grasp locations from an image is trained via supervised learning, using synthetic images for the training set. We demonstrate this approach on two robotic ma- nipulation platforms. Our algorithm successfully grasps a wide variety of objects, such as plates, tape-rolls, jugs, cellphones, keys, screwdrivers, sta- plers, a thick coil of wire, a strangely shaped power horn, and others, none of which were seen in the training set. We also apply our method to the task of unloading items from dishwashers.1},
	journal = {I. J. Robotic Res.},
	author = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew},
	month = feb,
	year = {2008},
	pages = {157--173},
	file = {Submitted Version:/home/andrej/Zotero/storage/32FQ3QHQ/Saxena et al. - 2008 - Robotic Grasping of Novel Objects using Vision.pdf:application/pdf},
}

@inproceedings{kumra_robotic_2017,
	title = {Robotic grasp detection using deep convolutional neural networks},
	doi = {10.1109/IROS.2017.8202237},
	abstract = {Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21\% on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Kumra, S. and Kanan, C.},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Feature extraction, Grippers, Machine learning, Robot kinematics, Training},
	pages = {769--776},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/EWMUWI87/Kumra and Kanan - 2017 - Robotic grasp detection using deep convolutional n.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/82UGHYNC/8202237.html:text/html},
}

@article{lenz_deep_2015,
	title = {Deep learning for detecting robotic grasps},
	volume = {34},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364914549607},
	doi = {10.1177/0278364914549607},
	abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
	language = {en},
	number = {4-5},
	urldate = {2021-04-19},
	journal = {The International Journal of Robotics Research},
	author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
	month = apr,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {3D feature learning, Baxter, deep learning, PR2, RGB-D multi-modal data, Robotic grasping},
	pages = {705--724},
	file = {Accepted Version:/home/andrej/Zotero/storage/5MB6DAU6/Lenz et al. - 2015 - Deep learning for detecting robotic grasps.pdf:application/pdf},
}

@inproceedings{redmon_real-time_2015,
	title = {Real-time grasp detection using convolutional neural networks},
	doi = {10.1109/ICRA.2015.7139361},
	abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Redmon, J. and Angelova, A.},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {Robot kinematics, Training, Accuracy, Computer architecture, Measurement, Predictive models},
	pages = {1316--1322},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/DPJM3I9G/Redmon and Angelova - 2015 - Real-time grasp detection using convolutional neur.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/SW96SLLU/7139361.html:text/html},
}

@article{pinto_supersizing_2015,
	title = {Supersizing {Self}-supervision: {Learning} to {Grasp} from {50K} {Tries} and 700 {Robot} {Hours}},
	shorttitle = {Supersizing {Self}-supervision},
	abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
	author = {Pinto, Lerrel and Gupta, Abhinav},
	month = sep,
	year = {2015},
}

@article{ten_pas_grasp_2017,
	title = {Grasp {Pose} {Detection} in {Point} {Clouds}},
	volume = {36},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364917735594},
	doi = {10.1177/0278364917735594},
	abstract = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75\% and 95\% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real-world grasping. This paper proposes a number of innovations that together result in an improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93\% end-to-end grasp success rate for novel objects presented in dense clutter.},
	language = {en},
	number = {13-14},
	urldate = {2021-04-19},
	journal = {The International Journal of Robotics Research},
	author = {ten Pas, Andreas and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
	month = dec,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {grasp detection, grasping, manipulation, perception},
	pages = {1455--1473},
	file = {Submitted Version:/home/andrej/Zotero/storage/TECQFXRC/ten Pas et al. - 2017 - Grasp Pose Detection in Point Clouds.pdf:application/pdf},
}

@article{morrison_closing_2018,
	title = {Closing the {Loop} for {Robotic} {Grasping}: {A} {Real}-time, {Generative} {Grasp} {Synthesis} {Approach}},
	shorttitle = {Closing the {Loop} for {Robotic} {Grasping}},
	url = {http://arxiv.org/abs/1804.05172},
	abstract = {This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The light-weight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83\% grasp success rate on a set of previously unseen objects with adversarial geometry and 88\% on a set of household objects that are moved during the grasp attempt. We also achieve 81\% accuracy when grasping in dynamic clutter.},
	urldate = {2021-04-20},
	journal = {arXiv:1804.05172 [cs]},
	author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
	month = may,
	year = {2018},
	note = {arXiv: 1804.05172},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/JUC844ND/Morrison et al. - 2018 - Closing the Loop for Robotic Grasping A Real-time.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/ELYQIY5J/1804.html:text/html},
}

@article{lundell_robust_2019,
	title = {Robust {Grasp} {Planning} {Over} {Uncertain} {Shape} {Completions}},
	url = {http://arxiv.org/abs/1903.00645},
	doi = {10.1109/IROS40897.2019.8967816},
	abstract = {We present a method for planning robust grasps over uncertain shape completed objects. For shape completion, a deep neural network is trained to take a partial view of the object as input and outputs the completed shape as a voxel grid. The key part of the network is dropout layers which are enabled not only during training but also at run-time to generate a set of shape samples representing the shape uncertainty through Monte Carlo sampling. Given the set of shape completed objects, we generate grasp candidates on the mean object shape but evaluate them based on their joint performance in terms of analytical grasp metrics on all the shape candidates. We experimentally validate and benchmark our method against another state-of-the-art method with a Barrett hand on 90000 grasps in simulation and 200 grasps on a real Franka Emika Panda. All experimental results show statistically significant improvements both in terms of grasp quality metrics and grasp success rate, demonstrating that planning shape-uncertainty-aware grasps brings significant advantages over solely planning on a single shape estimate, especially when dealing with complex or unknown objects.},
	urldate = {2021-04-20},
	journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
	month = nov,
	year = {2019},
	note = {arXiv: 1903.00645},
	keywords = {Computer Science - Robotics},
	pages = {1526--1532},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/4IIMAXEA/Lundell et al. - 2019 - Robust Grasp Planning Over Uncertain Shape Complet.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/2QNJGDXE/1903.html:text/html},
}

@article{mahler_dex-net_2017,
	title = {Dex-{Net} 2.0: {Deep} {Learning} to {Plan} {Robust} {Grasps} with {Synthetic} {Point} {Clouds} and {Analytic} {Grasp} {Metrics}},
	shorttitle = {Dex-{Net} 2.0},
	url = {http://arxiv.org/abs/1703.09312},
	abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
	urldate = {2021-04-20},
	journal = {arXiv:1703.09312 [cs]},
	author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
	month = aug,
	year = {2017},
	note = {arXiv: 1703.09312},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/LU9ELL42/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/A67FB2GA/1703.html:text/html},
}

@article{mahler_dex-net_2018,
	title = {Dex-{Net} 3.0: {Computing} {Robust} {Robot} {Vacuum} {Suction} {Grasp} {Targets} in {Point} {Clouds} using a {New} {Analytic} {Model} and {Deep} {Learning}},
	shorttitle = {Dex-{Net} 3.0},
	url = {http://arxiv.org/abs/1709.06670},
	abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98\${\textbackslash}\%\$, 82\${\textbackslash}\%\$, and 58\${\textbackslash}\%\$ respectively, improving to 81\${\textbackslash}\%\$ in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
	urldate = {2021-04-20},
	journal = {arXiv:1709.06670 [cs]},
	author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
	month = apr,
	year = {2018},
	note = {arXiv: 1709.06670},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/T3EY2QKF/Mahler et al. - 2018 - Dex-Net 3.0 Computing Robust Robot Vacuum Suction.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/5LSEHEE9/1709.html:text/html},
}

@article{mahler_learning_2019,
	title = {Learning ambidextrous robot grasping policies},
	volume = {4},
	doi = {10.1126/scirobotics.aau4984},
	abstract = {Universal picking (UP), or reliable robot grasping of a diverse range of novel objects from heaps, is a grand challenge for e-commerce order fulfillment, manufacturing, inspection, and home service robots. Optimizing the rate, reliability, and range of UP is difficult due to inherent uncertainty in sensing, control, and contact physics. This paper explores “ambidextrous” robot grasping, where two or more heterogeneous grippers are used. We present Dexterity Network (Dex-Net) 4.0, a substantial extension to previous versions of Dex-Net that learns policies for a given set of grippers by training on synthetic datasets using domain randomization with analytic models of physics and geometry. We train policies for a parallel-jaw and a vacuum-based suction cup gripper on 5 million synthetic depth images, grasps, and rewards generated from heaps of three-dimensional objects. On a physical robot with two grippers, the Dex-Net 4.0 policy consistently clears bins of up to 25 novel objects with reliability greater than 95\% at a rate of more than 300 mean picks per hour.},
	journal = {Science Robotics},
	author = {Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and Goldberg, Kenneth},
	month = jan,
	year = {2019},
	pages = {eaau4984},
	file = {Full Text:/home/andrej/Zotero/storage/7UL53HME/Mahler et al. - 2019 - Learning ambidextrous robot grasping policies.pdf:application/pdf},
}

@article{osa_algorithmic_2018,
	title = {An {Algorithmic} {Perspective} on {Imitation} {Learning}},
	volume = {7},
	doi = {10.1561/2300000053},
	abstract = {As robots and other intelligent agents move from simple environments and problems to more complex, unstructured settings, manually programming their behavior has become increasingly challenging and expensive. Often, it is easier for a teacher to demonstrate a desired behavior rather than attempt to manually engineer it. This process of learning from demonstrations, and the study of algorithms to do so, is called imitation learning. This work provides an introduction to imitation learning. It covers the underlying assumptions, approaches, and how they relate; the rich set of algorithms developed to tackle the problem; and advice on effective tools and implementation. We intend this paper to serve two audiences. First, we want to familiarize machine learning experts with the challenges of imitation learning, particularly those arising in robotics, and the interesting theoretical and practical distinctions between it and more familiar frameworks like statistical supervised learning theory and reinforcement learning. Second, we want to give roboticists and experts in applied artificial intelligence a broader appreciation for the frameworks and tools available for imitation learning.},
	journal = {Foundations and Trends in Robotics},
	author = {Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J. and Abbeel, Pieter and Peters, Jan},
	month = nov,
	year = {2018},
	pages = {1--179},
	file = {Full Text PDF:/home/andrej/Zotero/storage/648VP5IV/Osa et al. - 2018 - An Algorithmic Perspective on Imitation Learning.pdf:application/pdf},
}

@inproceedings{zhang_deep_2018,
	title = {Deep {Imitation} {Learning} for {Complex} {Manipulation} {Tasks} from {Virtual} {Reality} {Teleoperation}},
	doi = {10.1109/ICRA.2018.8461249},
	abstract = {Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Grippers, Head, Neural networks, Robots, Task analysis, Three-dimensional displays, Visualization},
	pages = {5628--5635},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/JJV9P4ZJ/Zhang et al. - 2018 - Deep Imitation Learning for Complex Manipulation T.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/L56F4XHY/8461249.html:text/html},
}

@article{polydoros_survey_2017,
	title = {Survey of {Model}-{Based} {Reinforcement} {Learning}: {Applications} on {Robotics}},
	volume = {86},
	issn = {1573-0409},
	shorttitle = {Survey of {Model}-{Based} {Reinforcement} {Learning}},
	url = {https://doi.org/10.1007/s10846-017-0468-y},
	doi = {10.1007/s10846-017-0468-y},
	abstract = {Reinforcement learning is an appealing approach for allowing robots to learn new tasks. Relevant literature reveals a plethora of methods, but at the same time makes clear the lack of implementations for dealing with real life challenges. Current expectations raise the demand for adaptable robots. We argue that, by employing model-based reinforcement learning, the—now limited—adaptability characteristics of robotic systems can be expanded. Also, model-based reinforcement learning exhibits advantages that makes it more applicable to real life use-cases compared to model-free methods. Thus, in this survey, model-based methods that have been applied in robotics are covered. We categorize them based on the derivation of an optimal policy, the definition of the returns function, the type of the transition model and the learned task. Finally, we discuss the applicability of model-based reinforcement learning approaches in new applications, taking into consideration the state of the art in both algorithms and hardware.},
	language = {en},
	number = {2},
	urldate = {2021-04-23},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Polydoros, Athanasios S. and Nalpantidis, Lazaros},
	month = may,
	year = {2017},
	pages = {153--173},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/XP2ZRRIM/Polydoros and Nalpantidis - 2017 - Survey of Model-Based Reinforcement Learning Appl.pdf:application/pdf},
}

@inproceedings{deisenroth_pilco_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {{PILCO}: a model-based and data-efficient approach to policy search},
	isbn = {978-1-4503-0619-5},
	shorttitle = {{PILCO}},
	abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
	urldate = {2021-04-23},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	month = jun,
	year = {2011},
	pages = {465--472},
}

@incollection{durrant-whyte_learning_2012,
	title = {Learning to {Control} a {Low}-{Cost} {Manipulator} {Using} {Data}-{Efficient} {Reinforcement} {Learning}},
	isbn = {978-0-262-30596-9},
	url = {http://ieeexplore.ieee.org/document/6301026},
	abstract = {Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials—from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.},
	urldate = {2021-04-23},
	booktitle = {Robotics: {Science} and {Systems} {VII}},
	publisher = {MIT Press},
	author = {Durrant-Whyte, Hugh and Roy, Nicholas and Abbeel, Pieter},
	year = {2012},
	note = {Conference Name: Robotics: Science and Systems VII},
	pages = {57--64},
	file = {IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/H6E84ZDV/6301026.html:text/html},
}

@inproceedings{quillen_deep_2018,
	title = {Deep {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Grasping}: {A} {Simulated} {Comparative} {Evaluation} of {Off}-{Policy} {Methods}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Grasping}},
	doi = {10.1109/ICRA.2018.8461039},
	abstract = {In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms1.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Quillen, Deirdre and Jang, Eric and Nachum, Ofir and Finn, Chelsea and Ibarz, Julian and Levine, Sergey},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Machine learning, Training, Robots, Task analysis, Benchmark testing, Grasping, Monte Carlo methods},
	pages = {6284--6291},
	file = {IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/BLZPWACV/8461039.html:text/html;Submitted Version:/home/andrej/Zotero/storage/5XVJVXCW/Quillen et al. - 2018 - Deep Reinforcement Learning for Vision-Based Robot.pdf:application/pdf},
}

@article{levine_learning_2016,
	title = {Learning {Hand}-{Eye} {Coordination} for {Robotic} {Grasping} with {Deep} {Learning} and {Large}-{Scale} {Data} {Collection}},
	volume = {37},
	doi = {10.1177/0278364917710318},
	abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
	journal = {The International Journal of Robotics Research},
	author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
	month = mar,
	year = {2016},
	file = {Full Text:/home/andrej/Zotero/storage/QFVUI2ZT/Levine et al. - 2016 - Learning Hand-Eye Coordination for Robotic Graspin.pdf:application/pdf},
}

@article{breyer_comparing_2019,
	title = {Comparing {Task} {Simplifications} to {Learn} {Closed}-{Loop} {Object} {Picking} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {PP},
	doi = {10.1109/LRA.2019.2896467},
	abstract = {Enabling autonomous robots to interact in unstructured environments with dynamic objects requires manipulation capabilities that can deal with clutter, changes, and objects' variability. This paper presents a comparison of different reinforcement learning-based approaches for object picking with a robotic manipulator. We learn closed-loop policies mapping depth camera inputs to motion commands and compare different approaches to keep the problem tractable, including reward shaping, curriculum learning and using a policy pre-trained on a task with a reduced action set to warm- start the full problem. For efficient and more flexible data collection, we train in simulation and transfer the policies to a real robot. We show that using curriculum learning, policies learned with a sparse reward formulation can be trained at similar rates as with a shaped reward. These policies result in success rates comparable to the policy initialized on the simplified task. We could successfully transfer these policies to the real robot with only minor modifications of the depth image filtering. We found that using a heuristic to warm-start the training was useful to enforce desired behavior, while the policies trained from scratch using a curriculum learned better to cope with unseen scenarios where objects are removed.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Breyer, Michel and Furrer, Fadri and Novkovic, Tonci and Siegwart, Roland and Nieto, Juan},
	month = jan,
	year = {2019},
	pages = {1--1},
	file = {Submitted Version:/home/andrej/Zotero/storage/CP2A93X6/Breyer et al. - 2019 - Comparing Task Simplifications to Learn Closed-Loo.pdf:application/pdf},
}

@book{kim_acceleration_2020,
	title = {Acceleration of {Actor}-{Critic} {Deep} {Reinforcement} {Learning} for {Visual} {Grasping} in {Clutter} by {State} {Representation} {Learning} {Based} on {Disentanglement} of a {Raw} {Input} {Image}},
	abstract = {For a robotic grasping task in which diverse unseen target objects exist in a cluttered environment, some deep learning-based methods have achieved state-of-the-art results using visual input directly. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when grasping diverse objects, especially when learning from raw images and sparse rewards. To make these RL techniques feasible for vision-based grasping tasks, we employ state representation learning (SRL), where we encode essential information first for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning the grasping skill, because the visual inputs for representation learning, where a robot attempts to grasp a target object in clutter, are extremely complex. We found that preprocessing based on the disentanglement of a raw input image is the key to effectively capturing a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. We demonstrate the effectiveness of this approach with varying levels of disentanglement in a realistic simulated environment.},
	author = {Kim, Taewon and Park, Yeseong and Park, Youngbin and Suh, Il Hong},
	month = feb,
	year = {2020},
}

@inproceedings{joshi_robotic_2020,
	title = {Robotic {Grasping} using {Deep} {Reinforcement} {Learning}},
	doi = {10.1109/CASE48305.2020.9216986},
	abstract = {In this work, we present a deep reinforcement learning based method to solve the problem of robotic grasping using visio-motor feedback. The use of a deep learning based approach reduces the complexity caused by the use of hand-designed features. Our method uses an off-policy reinforcement learning framework to learn the grasping policy. We use the double deep Q-learning framework along with a novel GraspQ-Network to output grasp probabilities used to learn grasps that maximize the pick success. We propose a visual servoing mechanism that uses a multi-view camera setup that observes the scene which contains the objects of interest. We performed experiments using a Baxter Gazebo simulated environment as well as on the actual robot. The results show that our proposed method outperforms the baseline Q-learning framework and increases grasping accuracy by adapting a multi-view model in comparison to a single-view model.},
	booktitle = {2020 {IEEE} 16th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Joshi, Shirin and Kumra, Sulabh and Sahin, Ferat},
	month = aug,
	year = {2020},
	note = {ISSN: 2161-8089},
	keywords = {Grippers, Machine learning, Learning (artificial intelligence), Grasping, Cameras, Robot vision systems},
	pages = {1461--1466},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/7ATBJ497/Joshi et al. - 2020 - Robotic Grasping using Deep Reinforcement Learning.pdf:application/pdf},
}

@book{zhan_framework_2020,
	title = {A {Framework} for {Efficient} {Robotic} {Manipulation}},
	abstract = {Data-efficient learning of manipulation policies from visual observations is an outstanding challenge for real-robot learning. While deep reinforcement learning (RL) algorithms have shown success learning policies from visual observations, they still require an impractical number of real-world data samples to learn effective policies. However, recent advances in unsupervised representation learning and data augmentation significantly improved the sample efficiency of training RL policies on common simulated benchmarks. Building on these advances, we present a Framework for Efficient Robotic Manipulation (FERM) that utilizes data augmentation and unsupervised learning to achieve extremely sample-efficient training of robotic manipulation policies with sparse rewards. We show that, given only 10 demonstrations, a single robotic arm can learn sparse-reward manipulation policies from pixels, such as reaching, picking, moving, pulling a large object, flipping a switch, and opening a drawer in just 15-50 minutes of real-world training time. We include videos, code, and additional information on the project website -- https://sites.google.com/view/efficient-robotic-manipulation.},
	author = {Zhan, Albert and Zhao, Philip and Pinto, Lerrel and Abbeel, Pieter and Laskin, Michael},
	month = dec,
	year = {2020},
}

@inproceedings{haarnoja_composable_2018,
	title = {Composable {Deep} {Reinforcement} {Learning} for {Robotic} {Manipulation}},
	doi = {10.1109/ICRA.2018.8460756},
	abstract = {Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Machine learning, Training, Neural networks, Robots, Task analysis, Learning (artificial intelligence), Entropy},
	pages = {6244--6251},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/HQCBUJGA/Haarnoja et al. - 2018 - Composable Deep Reinforcement Learning for Robotic.pdf:application/pdf},
}

@inproceedings{gualtieri_pick_2018,
	title = {Pick and {Place} {Without} {Geometric} {Object} {Models}},
	doi = {10.1109/ICRA.2018.8460553},
	abstract = {We propose a novel formulation of robotic pick and place as a deep reinforcement learning (RL) problem. Whereas most deep RL approaches to robotic manipulation frame the problem in terms of low level states and actions, we propose a more abstract formulation. In this formulation, actions are target reach poses for the hand and states are a history of such reaches. We show this approach can solve a challenging class of pick-place and regrasping problems where the exact geometry of the objects to be handled is unknown. The only information our method requires is: 1) the sensor perception available to the robot at test time; 2) prior knowledge of the general class of objects for which the system was trained. We evaluate our method using objects belonging to two different categories, mugs and bottles, both in simulation and on real hardware. Results show a major improvement relative to a shape primitives baseline.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Gualtieri, Marcus and Pas, Andreas ten and Platt, Robert},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Task analysis, Three-dimensional displays, Geometry, History, Robot sensing systems, Shape},
	pages = {7433--7440},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/XIZ7KWXF/Gualtieri et al. - 2018 - Pick and Place Without Geometric Object Models.pdf:application/pdf},
}

@article{wu_generative_2020,
	title = {Generative {Attention} {Learning}: a “{GenerAL}” framework for high-performance multi-fingered grasping in clutter},
	volume = {44},
	issn = {1573-7527},
	shorttitle = {Generative {Attention} {Learning}},
	url = {https://doi.org/10.1007/s10514-020-09907-y},
	doi = {10.1007/s10514-020-09907-y},
	abstract = {Generative Attention Learning (GenerAL) is a framework for high-DOF multi-fingered grasping that is not only robust to dense clutter and novel objects but also effective with a variety of different parallel-jaw and multi-fingered robot hands. This framework introduces a novel attention mechanism that substantially improves the grasp success rate in clutter. Its generative nature allows the learning of full-DOF grasps with flexible end-effector positions and orientations, as well as all finger joint angles of the hand. Trained purely in simulation, this framework skillfully closes the sim-to-real gap. To close the visual sim-to-real gap, this framework uses a single depth image as input. To close the dynamics sim-to-real gap, this framework circumvents continuous motor control with a direct mapping from pixel to Cartesian space inferred from the same depth image. Finally, this framework demonstrates inter-robot generality by achieving over \$\$92{\textbackslash}\%\$\$real-world grasp success rates in cluttered scenes with novel objects using two multi-fingered robotic hand-arm systems with different degrees of freedom.},
	language = {en},
	number = {6},
	urldate = {2021-04-24},
	journal = {Autonomous Robots},
	author = {Wu, Bohan and Akinola, Iretiayo and Gupta, Abhi and Xu, Feng and Varley, Jacob and Watkins-Valls, David and Allen, Peter K.},
	month = jul,
	year = {2020},
	pages = {971--990},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/K6Y2TZIB/Wu et al. - 2020 - Generative Attention Learning a “GenerAL” framewo.pdf:application/pdf},
}

@inproceedings{liu_active_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Active {Affordance} {Exploration} for {Robot} {Grasping}},
	isbn = {978-3-030-27541-9},
	doi = {10.1007/978-3-030-27541-9_35},
	abstract = {Robotic grasp in complicated un-structured warehouse environments is still a challenging task and attracts lots of attentions from robot vision and machine learning communities. A popular strategy is to directly detect the graspable region for specific end-effector such as suction cup, two-fingered gripper or multi-fingered hand. However, those work usually depends on the accurate object detection and precise pose estimation. Very recently, affordance map which describes the action possibilities that an environment can offer, begins to be used for grasp tasks. But it often fails in cluttered environments and degrades the efficiency of warehouse automation. In this paper, we establish an active exploration framework for robot grasp and design a deep reinforcement learning method. To verify the effectiveness, we develop a new composite hand which combines the suction cup and fingers and the experimental validations on robotic grasp tasks show the advantages of the active exploration method. This novel method significantly improves the grasp efficiency of the warehouse manipulators.},
	language = {en},
	booktitle = {Intelligent {Robotics} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Liu, Huaping and Yuan, Yuan and Deng, Yuhong and Guo, Xiaofeng and Wei, Yixuan and Lu, Kai and Fang, Bin and Guo, Di and Sun, Fuchun},
	editor = {Yu, Haibin and Liu, Jinguo and Liu, Lianqing and Ju, Zhaojie and Liu, Yuwang and Zhou, Dalin},
	year = {2019},
	keywords = {Active exploration, Affordance map, Robotic grasp},
	pages = {426--438},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/VR477GAA/Liu et al. - 2019 - Active Affordance Exploration for Robot Grasping.pdf:application/pdf},
}

@inproceedings{zeng_learning_2018,
	title = {Learning {Synergies} {Between} {Pushing} and {Grasping} with {Self}-{Supervised} {Deep} {Reinforcement} {Learning}},
	doi = {10.1109/IROS.2018.8593986},
	abstract = {Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end-effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors even amid challenging cases of tightly packed clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu/},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
	month = oct,
	year = {2018},
	note = {ISSN: 2153-0866},
	keywords = {Training, Three-dimensional displays, Grasping, Manipulators, Planning, Reinforcement learning},
	pages = {4238--4245},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/R4MZFJV8/Zeng et al. - 2018 - Learning Synergies Between Pushing and Grasping wi.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/7PPDTIQ5/8593986.html:text/html},
}

@article{popov_data-efficient_2017,
	title = {Data-efficient {Deep} {Reinforcement} {Learning} for {Dexterous} {Manipulation}},
	url = {http://arxiv.org/abs/1704.03073},
	abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
	urldate = {2021-04-24},
	journal = {arXiv:1704.03073 [cs]},
	author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.03073},
	keywords = {Computer Science - Robotics, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/M8XTRSQ9/Popov et al. - 2017 - Data-efficient Deep Reinforcement Learning for Dex.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/GBCZYQWD/1704.html:text/html},
}

@inproceedings{tobin_domain_2017,
	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
	doi = {10.1109/IROS.2017.8202133},
	abstract = {Bridging the `reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Training, Robots, Three-dimensional displays, Cameras, Adaptation models, Data models, Solid modeling},
	pages = {23--30},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/JRM7MRPZ/Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf:application/pdf},
}

@article{islam_reproducibility_2017,
	title = {Reproducibility of {Benchmarked} {Deep} {Reinforcement} {Learning} {Tasks} for {Continuous} {Control}},
	abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
	author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
	month = aug,
	year = {2017},
	file = {Full Text PDF:/home/andrej/Zotero/storage/H6XYUW7U/Islam et al. - 2017 - Reproducibility of Benchmarked Deep Reinforcement .pdf:application/pdf},
}

@inproceedings{gu_deep_2017,
	title = {Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates},
	doi = {10.1109/ICRA.2017.7989385},
	abstract = {Reinforcement learning holds the promise of enabling autonomous robots to learn large repertoires of behavioral skills with minimal human intervention. However, robotic applications of reinforcement learning often compromise the autonomy of the learning process in favor of achieving training times that are practical for real physical systems. This typically involves introducing hand-engineered policy representations and human-supplied demonstrations. Deep reinforcement learning alleviates this limitation by training general-purpose neural network policies, but applications of direct deep reinforcement learning algorithms have so far been restricted to simulated settings and relatively simple tasks, due to their apparent high sample complexity. In this paper, we demonstrate that a recent deep reinforcement learning algorithm based on off-policy training of deep Q-functions can scale to complex 3D manipulation tasks and can learn deep neural network policies efficiently enough to train on real physical robots. We demonstrate that the training times can be further reduced by parallelizing the algorithm across multiple robots which pool their policy updates asynchronously. Our experimental evaluation shows that our method can learn a variety of 3D manipulation skills in simulation and a complex door opening skill on real robots without any prior demonstrations or manually designed representations.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
	month = may,
	year = {2017},
	keywords = {Heuristic algorithms, Training, Neural networks, Robots, Learning (artificial intelligence), Instruction sets, Safety},
	pages = {3389--3396},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/3LCGAXSA/Gu et al. - 2017 - Deep reinforcement learning for robotic manipulati.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/8EM2B3K7/7989385.html:text/html},
}

@book{kalashnikov_qt-opt_2018,
	title = {{QT}-{Opt}: {Scalable} {Deep} {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Manipulation}},
	shorttitle = {{QT}-{Opt}},
	abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
	author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
	month = jun,
	year = {2018},
	file = {Full Text PDF:/home/andrej/Zotero/storage/5GWJ9TCP/Kalashnikov et al. - 2018 - QT-Opt Scalable Deep Reinforcement Learning for V.pdf:application/pdf},
}

@book{gualtieri_learning_2018,
	title = {Learning 6-{DoF} {Grasping} and {Pick}-{Place} {Using} {Attention} {Focus}},
	abstract = {We address a class of manipulation problems where the robot perceives the scene with a depth sensor and can move its end effector in a space with six degrees of freedom -- 3D position and orientation. Our approach is to formulate the problem as a Markov decision process (MDP) with abstract yet generally applicable state and action representations. Finding a good solution to the MDP requires adding constraints on the allowed actions. We develop a specific set of constraints called hierarchical SE(3) sampling (HSE3S) which causes the robot to learn a sequence of gazes to focus attention on the task-relevant parts of the scene. We demonstrate the effectiveness of our approach on three challenging pick-place tasks (with novel objects in clutter and nontrivial places) both in simulation and on a real robot, even though all training is done in simulation.},
	author = {Gualtieri, Marcus and Platt, Robert},
	month = jun,
	year = {2018},
}

@inproceedings{osa_experiments_2017,
	address = {Cham},
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {Experiments with {Hierarchical} {Reinforcement} {Learning} of {Multiple} {Grasping} {Policies}},
	isbn = {978-3-319-50115-4},
	doi = {10.1007/978-3-319-50115-4_15},
	abstract = {Robotic grasping has attracted considerable interest, but it still remains a challenging task. The data-driven approach is a promising solution to the robotic grasping problem; this approach leverages a grasp dataset and generalizes grasps for various objects. However, these methods often depend on the quality of the given datasets, which are not trivial to obtain with sufficient quality. Although reinforcement learning approaches have been recently used to achieve autonomous collection of grasp datasets, the existing algorithms are often limited to specific grasp types. In this paper, we present a framework for hierarchical reinforcement learning of grasping policies. In our framework, the lower-level hierarchy learns multiple grasp types, and the upper-level hierarchy learns a policy to select from the learned grasp types according to a point cloud of a new object. Through experiments, we validate that our approach learns grasping by constructing the grasp dataset autonomously. The experimental results show that our approach learns multiple grasping policies and generalizes the learned grasps by using local point cloud information.},
	language = {en},
	booktitle = {2016 {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Osa, Takayuki and Peters, Jan and Neumann, Gerhard},
	editor = {Kulić, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
	year = {2017},
	keywords = {Grasping, Point clouds, Reinforcement learning},
	pages = {160--172},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/KXVZZK7X/Osa et al. - 2017 - Experiments with Hierarchical Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{nguyen_review_2019,
	title = {Review of {Deep} {Reinforcement} {Learning} for {Robot} {Manipulation}},
	doi = {10.1109/IRC.2019.00120},
	abstract = {Reinforcement learning combined with neural networks has recently led to a wide range of successes in learning policies in different domains. For robot manipulation, reinforcement learning algorithms bring the hope for machines to have the human-like abilities by directly learning dexterous manipulation from raw pixels. In this review paper, we address the current status of reinforcement learning algorithms used in the field. We also cover essential theoretical background and main issues with current algorithms, which are limiting their applications of reinforcement learning algorithms in solving practical problems in robotics. We also share our thoughts on a number of future directions for reinforcement learning research.},
	booktitle = {2019 {Third} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Nguyen, Hai and La, Hung},
	month = feb,
	year = {2019},
	keywords = {Deep reinforcement learning, Optimization, Reinforcement learning, Robot manipulation, Robot sensing systems, Task analysis, Taxonomy, Training},
	pages = {590--595},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/IBGU5SZJ/Nguyen and La - 2019 - Review of Deep Reinforcement Learning for Robot Ma.pdf:application/pdf},
}

@inproceedings{henderson_deep_2018,
	title = {Deep {Reinforcement} {Learning} that {Matters}},
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	booktitle = {{AAAI}},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, D.},
	year = {2018},
}
