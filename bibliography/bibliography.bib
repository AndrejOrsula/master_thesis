
@book{sutton_reinforcement_2018,
	address = {Cambridge, MA, USA},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	publisher = {A Bradford Book},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	pages = {529--533},
	file = {Snapshot:/home/andrej/Zotero/storage/KZVHNVLP/nature14236.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Submitted Version:/home/andrej/Zotero/storage/6IMRYRPU/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@article{schrittwieser_mastering_2020,
	title = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
	volume = {588},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03051-4},
	doi = {10.1038/s41586-020-03051-4},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	language = {en},
	number = {7839},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = dec,
	year = {2020},
	note = {Number: 7839
Publisher: Nature Publishing Group},
	pages = {604--609},
	file = {Snapshot:/home/andrej/Zotero/storage/BE6MS6T2/s41586-020-03051-4.html:text/html;Submitted Version:/home/andrej/Zotero/storage/JATC7FIM/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf:application/pdf},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	pages = {350--354},
	file = {Snapshot:/home/andrej/Zotero/storage/5SDL7JC8/s41586-019-1724-z.html:text/html},
}

@article{sahbani_overview_2012,
	series = {Autonomous {Grasping}},
	title = {An overview of {3D} object grasp synthesis algorithms},
	volume = {60},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889011001485},
	doi = {10.1016/j.robot.2011.07.016},
	abstract = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger–object contact interactions Bicchi and Kumar (2000) [12] or robot hand design and their control Al-Gallaf et al. (1993) [70]. Robot grasp synthesis algorithms have been reviewed in Shimoga (1996) [71], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
	language = {en},
	number = {3},
	urldate = {2021-04-18},
	journal = {Robotics and Autonomous Systems},
	author = {Sahbani, A. and El-Khoury, S. and Bidaud, P.},
	month = mar,
	year = {2012},
	keywords = {Force-closure, Grasp synthesis, Learning by demonstration, Task modeling},
	pages = {326--336},
	file = {ScienceDirect Full Text PDF:/home/andrej/Zotero/storage/9QSV9H4L/Sahbani et al. - 2012 - An overview of 3D object grasp synthesis algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/andrej/Zotero/storage/62GHM7TM/S0921889011001485.html:text/html},
}

@article{kroemer_review_2021,
	title = {A {Review} of {Robot} {Learning} for {Manipulation}: {Challenges}, {Representations}, and {Algorithms}},
	shorttitle = {A {Review} of {Robot} {Learning} for {Manipulation}},
	abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
	journal = {J. Mach. Learn. Res.},
	author = {Kroemer, Oliver and Niekum, S. and Konidaris, G.},
	year = {2021},
	file = {Full Text PDF:/home/andrej/Zotero/storage/SFIZLJYN/Kroemer et al. - 2021 - A Review of Robot Learning for Manipulation Chall.pdf:application/pdf},
}

@article{yun-hui_liu_complete_2004,
	title = {A complete and efficient algorithm for searching 3-{D} form-closure grasps in the discrete domain},
	volume = {20},
	issn = {1941-0468},
	doi = {10.1109/TRO.2004.829500},
	abstract = {A complete and efficient algorithm is proposed for searching form-closure grasps of n hard fingers on the surface of a three-dimensional object represented by discrete points. Both frictional and frictionless cases are considered. This algorithm starts to search a form-closure grasp from a randomly selected grasp using an efficient local search procedure until encountering a local minimum. The local search procedure employs the powerful ray-shooting technique to search in the direction of reducing the distance between the convex hull corresponding to the grasp and the origin of the wrench space. When the distance reaches a local minimum in the local search procedure, the algorithm decomposes the problem into a few subproblems in subsets of the points according to the existence conditions of form-closure grasps. A search tree whose root represents the original problem is employed to perform the searching process. The subproblems are represented as children of the root node and the same procedure is recursively applied to the children. It is proved that the search tree generates O(KlnK/n) nodes in case a from-closure grasp exists, where K is the number of the local minimum points of the distance in the grasp space and n is the number of fingers. Compared to the exhaustive search, this algorithm is more efficient, and, compared to other heuristic algorithms, the proposed algorithm is complete in the discrete domain. The efficiency of this algorithm is demonstrated by numerical examples.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {{Yun-Hui Liu} and {Miu-Ling Lam} and Ding, D.},
	month = oct,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Automation, Computer science education, Councils, Educational programs, Educational technology, Fingers, Fixtures, Heuristic algorithms, Intelligent robots, Testing},
	pages = {805--816},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/B5VH23G6/Yun-Hui Liu et al. - 2004 - A complete and efficient algorithm for searching 3.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/BQRMWVGA/1339381.html:text/html},
}

@inproceedings{nguyen_constructing_1987,
	title = {Constructing stable grasps in {3D}},
	volume = {4},
	doi = {10.1109/ROBOT.1987.1088008},
	abstract = {This paper presents fast and simple algorithms for directly constructing stable grasps in 3D. The synthesis of stable grasps constructs virtual springs at the contacts, such that the grasped object is stable, and has a desired stiffness matrix about its stable equilibrium. The paper develops a simple geometric relation between the stiffness of the grasp and the spatial configuration of the virtual springs at the contacts. The stiffness of the grasp also depends on whether the points of contact stick, or slide without friction on the edges of the object.},
	booktitle = {1987 {IEEE} {International} {Conference} on {Robotics} and {Automation} {Proceedings}},
	author = {Nguyen, V.-},
	month = mar,
	year = {1987},
	keywords = {Fingers, Artificial intelligence, Friction, Paper technology, Research and development, Resists, Rubber, Springs, Stability, Tendons},
	pages = {234--239},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/6RUS2LUC/Nguyen - 1987 - Constructing stable grasps in 3D.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/A86EDLN3/1088008.html:text/html},
}

@article{roa_grasp_2015,
	title = {Grasp quality measures: review and performance},
	volume = {38},
	issn = {1573-7527},
	shorttitle = {Grasp quality measures},
	url = {https://doi.org/10.1007/s10514-014-9402-3},
	doi = {10.1007/s10514-014-9402-3},
	abstract = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
	language = {en},
	number = {1},
	urldate = {2021-04-19},
	journal = {Autonomous Robots},
	author = {Roa, Máximo A. and Suárez, Raúl},
	month = jan,
	year = {2015},
	pages = {65--88},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/7X5NQZ6W/Roa and Suárez - 2015 - Grasp quality measures review and performance.pdf:application/pdf},
}

@article{saxena_robotic_2008,
	title = {Robotic {Grasping} of {Novel} {Objects} using {Vision}},
	volume = {27},
	doi = {10.1177/0278364907087172},
	abstract = {We consider the problem of grasping novel objects, specifically ones that are being seen for the first time through vision. Grasping a previously un- known object, one for which a 3-d model is not available, is a challenging problem. Further, even if given a model, one still has to decide where to grasp the object. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Given two (or more) images of an ob- ject, our algorithm attempts to identify a few points in each image corresponding to good locations at which to grasp the object. This sparse set of points is then triangulated to obtain a 3-d location at which to attempt a grasp. This is in contrast to standard dense stereo, which tries to triangulate every single point in an image (and often fails to return a good 3-d model). Our algorithm for identifying grasp locations from an image is trained via supervised learning, using synthetic images for the training set. We demonstrate this approach on two robotic ma- nipulation platforms. Our algorithm successfully grasps a wide variety of objects, such as plates, tape-rolls, jugs, cellphones, keys, screwdrivers, sta- plers, a thick coil of wire, a strangely shaped power horn, and others, none of which were seen in the training set. We also apply our method to the task of unloading items from dishwashers.1},
	journal = {I. J. Robotic Res.},
	author = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew},
	month = feb,
	year = {2008},
	pages = {157--173},
	file = {Submitted Version:/home/andrej/Zotero/storage/32FQ3QHQ/Saxena et al. - 2008 - Robotic Grasping of Novel Objects using Vision.pdf:application/pdf},
}

@inproceedings{kumra_robotic_2017,
	title = {Robotic grasp detection using deep convolutional neural networks},
	doi = {10.1109/IROS.2017.8202237},
	abstract = {Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21\% on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Kumra, S. and Kanan, C.},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Feature extraction, Grippers, Machine learning, Robot kinematics, Training},
	pages = {769--776},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/EWMUWI87/Kumra and Kanan - 2017 - Robotic grasp detection using deep convolutional n.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/82UGHYNC/8202237.html:text/html},
}

@article{lenz_deep_2015,
	title = {Deep learning for detecting robotic grasps},
	volume = {34},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364914549607},
	doi = {10.1177/0278364914549607},
	abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
	language = {en},
	number = {4-5},
	urldate = {2021-04-19},
	journal = {The International Journal of Robotics Research},
	author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
	month = apr,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {3D feature learning, Baxter, deep learning, PR2, RGB-D multi-modal data, Robotic grasping},
	pages = {705--724},
	file = {Accepted Version:/home/andrej/Zotero/storage/5MB6DAU6/Lenz et al. - 2015 - Deep learning for detecting robotic grasps.pdf:application/pdf},
}

@inproceedings{redmon_real-time_2015,
	title = {Real-time grasp detection using convolutional neural networks},
	doi = {10.1109/ICRA.2015.7139361},
	abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Redmon, J. and Angelova, A.},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {Robot kinematics, Training, Accuracy, Computer architecture, Measurement, Predictive models},
	pages = {1316--1322},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/DPJM3I9G/Redmon and Angelova - 2015 - Real-time grasp detection using convolutional neur.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/SW96SLLU/7139361.html:text/html},
}

@article{pinto_supersizing_2015,
	title = {Supersizing {Self}-supervision: {Learning} to {Grasp} from {50K} {Tries} and 700 {Robot} {Hours}},
	shorttitle = {Supersizing {Self}-supervision},
	abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
	author = {Pinto, Lerrel and Gupta, Abhinav},
	month = sep,
	year = {2015},
}

@article{ten_pas_grasp_2017,
	title = {Grasp {Pose} {Detection} in {Point} {Clouds}},
	volume = {36},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364917735594},
	doi = {10.1177/0278364917735594},
	abstract = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75\% and 95\% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real-world grasping. This paper proposes a number of innovations that together result in an improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93\% end-to-end grasp success rate for novel objects presented in dense clutter.},
	language = {en},
	number = {13-14},
	urldate = {2021-04-19},
	journal = {The International Journal of Robotics Research},
	author = {ten Pas, Andreas and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
	month = dec,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {grasp detection, grasping, manipulation, perception},
	pages = {1455--1473},
	file = {Submitted Version:/home/andrej/Zotero/storage/TECQFXRC/ten Pas et al. - 2017 - Grasp Pose Detection in Point Clouds.pdf:application/pdf},
}

@article{morrison_closing_2018,
	title = {Closing the {Loop} for {Robotic} {Grasping}: {A} {Real}-time, {Generative} {Grasp} {Synthesis} {Approach}},
	shorttitle = {Closing the {Loop} for {Robotic} {Grasping}},
	url = {http://arxiv.org/abs/1804.05172},
	abstract = {This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The light-weight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83\% grasp success rate on a set of previously unseen objects with adversarial geometry and 88\% on a set of household objects that are moved during the grasp attempt. We also achieve 81\% accuracy when grasping in dynamic clutter.},
	urldate = {2021-04-20},
	journal = {arXiv:1804.05172 [cs]},
	author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
	month = may,
	year = {2018},
	note = {arXiv: 1804.05172},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/JUC844ND/Morrison et al. - 2018 - Closing the Loop for Robotic Grasping A Real-time.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/ELYQIY5J/1804.html:text/html},
}

@article{lundell_robust_2019,
	title = {Robust {Grasp} {Planning} {Over} {Uncertain} {Shape} {Completions}},
	url = {http://arxiv.org/abs/1903.00645},
	doi = {10.1109/IROS40897.2019.8967816},
	abstract = {We present a method for planning robust grasps over uncertain shape completed objects. For shape completion, a deep neural network is trained to take a partial view of the object as input and outputs the completed shape as a voxel grid. The key part of the network is dropout layers which are enabled not only during training but also at run-time to generate a set of shape samples representing the shape uncertainty through Monte Carlo sampling. Given the set of shape completed objects, we generate grasp candidates on the mean object shape but evaluate them based on their joint performance in terms of analytical grasp metrics on all the shape candidates. We experimentally validate and benchmark our method against another state-of-the-art method with a Barrett hand on 90000 grasps in simulation and 200 grasps on a real Franka Emika Panda. All experimental results show statistically significant improvements both in terms of grasp quality metrics and grasp success rate, demonstrating that planning shape-uncertainty-aware grasps brings significant advantages over solely planning on a single shape estimate, especially when dealing with complex or unknown objects.},
	urldate = {2021-04-20},
	journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
	month = nov,
	year = {2019},
	note = {arXiv: 1903.00645},
	keywords = {Computer Science - Robotics},
	pages = {1526--1532},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/4IIMAXEA/Lundell et al. - 2019 - Robust Grasp Planning Over Uncertain Shape Complet.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/2QNJGDXE/1903.html:text/html},
}

@article{mahler_dex-net_2017,
	title = {Dex-{Net} 2.0: {Deep} {Learning} to {Plan} {Robust} {Grasps} with {Synthetic} {Point} {Clouds} and {Analytic} {Grasp} {Metrics}},
	shorttitle = {Dex-{Net} 2.0},
	url = {http://arxiv.org/abs/1703.09312},
	abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
	urldate = {2021-04-20},
	journal = {arXiv:1703.09312 [cs]},
	author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
	month = aug,
	year = {2017},
	note = {arXiv: 1703.09312},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/LU9ELL42/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/A67FB2GA/1703.html:text/html},
}

@article{mahler_dex-net_2018,
	title = {Dex-{Net} 3.0: {Computing} {Robust} {Robot} {Vacuum} {Suction} {Grasp} {Targets} in {Point} {Clouds} using a {New} {Analytic} {Model} and {Deep} {Learning}},
	shorttitle = {Dex-{Net} 3.0},
	url = {http://arxiv.org/abs/1709.06670},
	abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98\${\textbackslash}\%\$, 82\${\textbackslash}\%\$, and 58\${\textbackslash}\%\$ respectively, improving to 81\${\textbackslash}\%\$ in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
	urldate = {2021-04-20},
	journal = {arXiv:1709.06670 [cs]},
	author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
	month = apr,
	year = {2018},
	note = {arXiv: 1709.06670},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/T3EY2QKF/Mahler et al. - 2018 - Dex-Net 3.0 Computing Robust Robot Vacuum Suction.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/5LSEHEE9/1709.html:text/html},
}

@article{mahler_learning_2019,
	title = {Learning ambidextrous robot grasping policies},
	volume = {4},
	doi = {10.1126/scirobotics.aau4984},
	abstract = {Universal picking (UP), or reliable robot grasping of a diverse range of novel objects from heaps, is a grand challenge for e-commerce order fulfillment, manufacturing, inspection, and home service robots. Optimizing the rate, reliability, and range of UP is difficult due to inherent uncertainty in sensing, control, and contact physics. This paper explores “ambidextrous” robot grasping, where two or more heterogeneous grippers are used. We present Dexterity Network (Dex-Net) 4.0, a substantial extension to previous versions of Dex-Net that learns policies for a given set of grippers by training on synthetic datasets using domain randomization with analytic models of physics and geometry. We train policies for a parallel-jaw and a vacuum-based suction cup gripper on 5 million synthetic depth images, grasps, and rewards generated from heaps of three-dimensional objects. On a physical robot with two grippers, the Dex-Net 4.0 policy consistently clears bins of up to 25 novel objects with reliability greater than 95\% at a rate of more than 300 mean picks per hour.},
	journal = {Science Robotics},
	author = {Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and Goldberg, Kenneth},
	month = jan,
	year = {2019},
	pages = {eaau4984},
	file = {Full Text:/home/andrej/Zotero/storage/7UL53HME/Mahler et al. - 2019 - Learning ambidextrous robot grasping policies.pdf:application/pdf},
}
