
@book{sutton_reinforcement_2018,
	address = {Cambridge, MA, USA},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	publisher = {A Bradford Book},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	pages = {529--533},
	file = {Snapshot:/home/andrej/Zotero/storage/KZVHNVLP/nature14236.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Submitted Version:/home/andrej/Zotero/storage/6IMRYRPU/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@article{schrittwieser_mastering_2020,
	title = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
	volume = {588},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03051-4},
	doi = {10.1038/s41586-020-03051-4},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	language = {en},
	number = {7839},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = dec,
	year = {2020},
	note = {Number: 7839
Publisher: Nature Publishing Group},
	pages = {604--609},
	file = {Snapshot:/home/andrej/Zotero/storage/BE6MS6T2/s41586-020-03051-4.html:text/html;Submitted Version:/home/andrej/Zotero/storage/JATC7FIM/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf:application/pdf},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	pages = {350--354},
	file = {Snapshot:/home/andrej/Zotero/storage/5SDL7JC8/s41586-019-1724-z.html:text/html},
}

@article{sahbani_overview_2012,
	series = {Autonomous {Grasping}},
	title = {An overview of {3D} object grasp synthesis algorithms},
	volume = {60},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889011001485},
	doi = {10.1016/j.robot.2011.07.016},
	abstract = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger–object contact interactions Bicchi and Kumar (2000) [12] or robot hand design and their control Al-Gallaf et al. (1993) [70]. Robot grasp synthesis algorithms have been reviewed in Shimoga (1996) [71], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
	language = {en},
	number = {3},
	urldate = {2021-04-18},
	journal = {Robotics and Autonomous Systems},
	author = {Sahbani, A. and El-Khoury, S. and Bidaud, P.},
	month = mar,
	year = {2012},
	keywords = {Force-closure, Grasp synthesis, Learning by demonstration, Task modeling},
	pages = {326--336},
	file = {ScienceDirect Full Text PDF:/home/andrej/Zotero/storage/9QSV9H4L/Sahbani et al. - 2012 - An overview of 3D object grasp synthesis algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/andrej/Zotero/storage/62GHM7TM/S0921889011001485.html:text/html},
}

@article{kroemer_review_2021,
	title = {A {Review} of {Robot} {Learning} for {Manipulation}: {Challenges}, {Representations}, and {Algorithms}},
	shorttitle = {A {Review} of {Robot} {Learning} for {Manipulation}},
	abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
	journal = {J. Mach. Learn. Res.},
	author = {Kroemer, Oliver and Niekum, S. and Konidaris, G.},
	year = {2021},
	file = {Full Text PDF:/home/andrej/Zotero/storage/SFIZLJYN/Kroemer et al. - 2021 - A Review of Robot Learning for Manipulation Chall.pdf:application/pdf},
}

@article{yun-hui_liu_complete_2004,
	title = {A complete and efficient algorithm for searching 3-{D} form-closure grasps in the discrete domain},
	volume = {20},
	issn = {1941-0468},
	doi = {10.1109/TRO.2004.829500},
	abstract = {A complete and efficient algorithm is proposed for searching form-closure grasps of n hard fingers on the surface of a three-dimensional object represented by discrete points. Both frictional and frictionless cases are considered. This algorithm starts to search a form-closure grasp from a randomly selected grasp using an efficient local search procedure until encountering a local minimum. The local search procedure employs the powerful ray-shooting technique to search in the direction of reducing the distance between the convex hull corresponding to the grasp and the origin of the wrench space. When the distance reaches a local minimum in the local search procedure, the algorithm decomposes the problem into a few subproblems in subsets of the points according to the existence conditions of form-closure grasps. A search tree whose root represents the original problem is employed to perform the searching process. The subproblems are represented as children of the root node and the same procedure is recursively applied to the children. It is proved that the search tree generates O(KlnK/n) nodes in case a from-closure grasp exists, where K is the number of the local minimum points of the distance in the grasp space and n is the number of fingers. Compared to the exhaustive search, this algorithm is more efficient, and, compared to other heuristic algorithms, the proposed algorithm is complete in the discrete domain. The efficiency of this algorithm is demonstrated by numerical examples.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {{Yun-Hui Liu} and {Miu-Ling Lam} and Ding, D.},
	month = oct,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Automation, Computer science education, Councils, Educational programs, Educational technology, Fingers, Fixtures, Heuristic algorithms, Intelligent robots, Testing},
	pages = {805--816},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/B5VH23G6/Yun-Hui Liu et al. - 2004 - A complete and efficient algorithm for searching 3.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/BQRMWVGA/1339381.html:text/html},
}

@inproceedings{nguyen_constructing_1987,
	title = {Constructing stable grasps in {3D}},
	volume = {4},
	doi = {10.1109/ROBOT.1987.1088008},
	abstract = {This paper presents fast and simple algorithms for directly constructing stable grasps in 3D. The synthesis of stable grasps constructs virtual springs at the contacts, such that the grasped object is stable, and has a desired stiffness matrix about its stable equilibrium. The paper develops a simple geometric relation between the stiffness of the grasp and the spatial configuration of the virtual springs at the contacts. The stiffness of the grasp also depends on whether the points of contact stick, or slide without friction on the edges of the object.},
	booktitle = {1987 {IEEE} {International} {Conference} on {Robotics} and {Automation} {Proceedings}},
	author = {Nguyen, V.-},
	month = mar,
	year = {1987},
	keywords = {Fingers, Artificial intelligence, Friction, Paper technology, Research and development, Resists, Rubber, Springs, Stability, Tendons},
	pages = {234--239},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/6RUS2LUC/Nguyen - 1987 - Constructing stable grasps in 3D.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/A86EDLN3/1088008.html:text/html},
}

@article{roa_grasp_2015,
	title = {Grasp quality measures: review and performance},
	volume = {38},
	issn = {1573-7527},
	shorttitle = {Grasp quality measures},
	url = {https://doi.org/10.1007/s10514-014-9402-3},
	doi = {10.1007/s10514-014-9402-3},
	abstract = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
	language = {en},
	number = {1},
	urldate = {2021-04-19},
	journal = {Autonomous Robots},
	author = {Roa, Máximo A. and Suárez, Raúl},
	month = jan,
	year = {2015},
	pages = {65--88},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/7X5NQZ6W/Roa and Suárez - 2015 - Grasp quality measures review and performance.pdf:application/pdf},
}

@article{saxena_robotic_2008,
	title = {Robotic {Grasping} of {Novel} {Objects} using {Vision}},
	volume = {27},
	doi = {10.1177/0278364907087172},
	abstract = {We consider the problem of grasping novel objects, specifically ones that are being seen for the first time through vision. Grasping a previously un- known object, one for which a 3-d model is not available, is a challenging problem. Further, even if given a model, one still has to decide where to grasp the object. We present a learning algorithm that neither requires, nor tries to build, a 3-d model of the object. Given two (or more) images of an ob- ject, our algorithm attempts to identify a few points in each image corresponding to good locations at which to grasp the object. This sparse set of points is then triangulated to obtain a 3-d location at which to attempt a grasp. This is in contrast to standard dense stereo, which tries to triangulate every single point in an image (and often fails to return a good 3-d model). Our algorithm for identifying grasp locations from an image is trained via supervised learning, using synthetic images for the training set. We demonstrate this approach on two robotic ma- nipulation platforms. Our algorithm successfully grasps a wide variety of objects, such as plates, tape-rolls, jugs, cellphones, keys, screwdrivers, sta- plers, a thick coil of wire, a strangely shaped power horn, and others, none of which were seen in the training set. We also apply our method to the task of unloading items from dishwashers.1},
	journal = {I. J. Robotic Res.},
	author = {Saxena, Ashutosh and Driemeyer, Justin and Ng, Andrew},
	month = feb,
	year = {2008},
	pages = {157--173},
	file = {Submitted Version:/home/andrej/Zotero/storage/32FQ3QHQ/Saxena et al. - 2008 - Robotic Grasping of Novel Objects using Vision.pdf:application/pdf},
}

@inproceedings{kumra_robotic_2017,
	title = {Robotic grasp detection using deep convolutional neural networks},
	doi = {10.1109/IROS.2017.8202237},
	abstract = {Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21\% on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Kumra, S. and Kanan, C.},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Feature extraction, Grippers, Machine learning, Robot kinematics, Training},
	pages = {769--776},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/EWMUWI87/Kumra and Kanan - 2017 - Robotic grasp detection using deep convolutional n.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/82UGHYNC/8202237.html:text/html},
}

@article{lenz_deep_2015,
	title = {Deep learning for detecting robotic grasps},
	volume = {34},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364914549607},
	doi = {10.1177/0278364914549607},
	abstract = {We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms.},
	language = {en},
	number = {4-5},
	urldate = {2021-04-19},
	journal = {The International Journal of Robotics Research},
	author = {Lenz, Ian and Lee, Honglak and Saxena, Ashutosh},
	month = apr,
	year = {2015},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {3D feature learning, Baxter, deep learning, PR2, RGB-D multi-modal data, Robotic grasping},
	pages = {705--724},
	file = {Accepted Version:/home/andrej/Zotero/storage/5MB6DAU6/Lenz et al. - 2015 - Deep learning for detecting robotic grasps.pdf:application/pdf},
}

@inproceedings{redmon_real-time_2015,
	title = {Real-time grasp detection using convolutional neural networks},
	doi = {10.1109/ICRA.2015.7139361},
	abstract = {We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.},
	booktitle = {2015 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Redmon, J. and Angelova, A.},
	month = may,
	year = {2015},
	note = {ISSN: 1050-4729},
	keywords = {Robot kinematics, Training, Accuracy, Computer architecture, Measurement, Predictive models},
	pages = {1316--1322},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/DPJM3I9G/Redmon and Angelova - 2015 - Real-time grasp detection using convolutional neur.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/SW96SLLU/7139361.html:text/html},
}

@article{pinto_supersizing_2015,
	title = {Supersizing {Self}-supervision: {Learning} to {Grasp} from {50K} {Tries} and 700 {Robot} {Hours}},
	shorttitle = {Supersizing {Self}-supervision},
	abstract = {Current learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping.},
	author = {Pinto, Lerrel and Gupta, Abhinav},
	month = sep,
	year = {2015},
}

@article{ten_pas_grasp_2017,
	title = {Grasp {Pose} {Detection} in {Point} {Clouds}},
	volume = {36},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364917735594},
	doi = {10.1177/0278364917735594},
	abstract = {Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75\% and 95\% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real-world grasping. This paper proposes a number of innovations that together result in an improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93\% end-to-end grasp success rate for novel objects presented in dense clutter.},
	language = {en},
	number = {13-14},
	urldate = {2021-04-19},
	journal = {The International Journal of Robotics Research},
	author = {ten Pas, Andreas and Gualtieri, Marcus and Saenko, Kate and Platt, Robert},
	month = dec,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {grasp detection, grasping, manipulation, perception},
	pages = {1455--1473},
	file = {Submitted Version:/home/andrej/Zotero/storage/TECQFXRC/ten Pas et al. - 2017 - Grasp Pose Detection in Point Clouds.pdf:application/pdf},
}

@article{morrison_closing_2018,
	title = {Closing the {Loop} for {Robotic} {Grasping}: {A} {Real}-time, {Generative} {Grasp} {Synthesis} {Approach}},
	shorttitle = {Closing the {Loop} for {Robotic} {Grasping}},
	url = {http://arxiv.org/abs/1804.05172},
	abstract = {This paper presents a real-time, object-independent grasp synthesis method which can be used for closed-loop grasping. Our proposed Generative Grasping Convolutional Neural Network (GG-CNN) predicts the quality and pose of grasps at every pixel. This one-to-one mapping from a depth image overcomes limitations of current deep-learning grasping techniques by avoiding discrete sampling of grasp candidates and long computation times. Additionally, our GG-CNN is orders of magnitude smaller while detecting stable grasps with equivalent performance to current state-of-the-art techniques. The light-weight and single-pass generative nature of our GG-CNN allows for closed-loop control at up to 50Hz, enabling accurate grasping in non-static environments where objects move and in the presence of robot control inaccuracies. In our real-world tests, we achieve an 83\% grasp success rate on a set of previously unseen objects with adversarial geometry and 88\% on a set of household objects that are moved during the grasp attempt. We also achieve 81\% accuracy when grasping in dynamic clutter.},
	urldate = {2021-04-20},
	journal = {arXiv:1804.05172 [cs]},
	author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
	month = may,
	year = {2018},
	note = {arXiv: 1804.05172},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/JUC844ND/Morrison et al. - 2018 - Closing the Loop for Robotic Grasping A Real-time.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/ELYQIY5J/1804.html:text/html},
}

@article{lundell_robust_2019,
	title = {Robust {Grasp} {Planning} {Over} {Uncertain} {Shape} {Completions}},
	url = {http://arxiv.org/abs/1903.00645},
	doi = {10.1109/IROS40897.2019.8967816},
	abstract = {We present a method for planning robust grasps over uncertain shape completed objects. For shape completion, a deep neural network is trained to take a partial view of the object as input and outputs the completed shape as a voxel grid. The key part of the network is dropout layers which are enabled not only during training but also at run-time to generate a set of shape samples representing the shape uncertainty through Monte Carlo sampling. Given the set of shape completed objects, we generate grasp candidates on the mean object shape but evaluate them based on their joint performance in terms of analytical grasp metrics on all the shape candidates. We experimentally validate and benchmark our method against another state-of-the-art method with a Barrett hand on 90000 grasps in simulation and 200 grasps on a real Franka Emika Panda. All experimental results show statistically significant improvements both in terms of grasp quality metrics and grasp success rate, demonstrating that planning shape-uncertainty-aware grasps brings significant advantages over solely planning on a single shape estimate, especially when dealing with complex or unknown objects.},
	urldate = {2021-04-20},
	journal = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	author = {Lundell, Jens and Verdoja, Francesco and Kyrki, Ville},
	month = nov,
	year = {2019},
	note = {arXiv: 1903.00645},
	keywords = {Computer Science - Robotics},
	pages = {1526--1532},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/4IIMAXEA/Lundell et al. - 2019 - Robust Grasp Planning Over Uncertain Shape Complet.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/2QNJGDXE/1903.html:text/html},
}

@article{mahler_dex-net_2017,
	title = {Dex-{Net} 2.0: {Deep} {Learning} to {Plan} {Robust} {Grasps} with {Synthetic} {Point} {Clouds} and {Analytic} {Grasp} {Metrics}},
	shorttitle = {Dex-{Net} 2.0},
	url = {http://arxiv.org/abs/1703.09312},
	abstract = {To reduce data collection time for deep learning of robust robotic grasp plans, we explore training from a synthetic dataset of 6.7 million point clouds, grasps, and analytic grasp metrics generated from thousands of 3D models from Dex-Net 1.0 in randomized poses on a table. We use the resulting dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network (GQ-CNN) model that rapidly predicts the probability of success of grasps from depth images, where grasps are specified as the planar position, angle, and depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000 trials on an ABB YuMi comparing grasp planning methods on singulated objects suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be used to plan grasps in 0.8sec with a success rate of 93\% on eight known objects with adversarial geometry and is 3x faster than registering point clouds to a precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp planner also has the highest success rate on a dataset of 10 novel rigid objects and achieves 99\% precision (one false positive out of 69 grasps classified as robust) on a dataset of 40 novel household objects, some of which are articulated or deformable. Code, datasets, videos, and supplementary material are available at http://berkeleyautomation.github.io/dex-net .},
	urldate = {2021-04-20},
	journal = {arXiv:1703.09312 [cs]},
	author = {Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken},
	month = aug,
	year = {2017},
	note = {arXiv: 1703.09312},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/LU9ELL42/Mahler et al. - 2017 - Dex-Net 2.0 Deep Learning to Plan Robust Grasps w.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/A67FB2GA/1703.html:text/html},
}

@article{mahler_dex-net_2018,
	title = {Dex-{Net} 3.0: {Computing} {Robust} {Robot} {Vacuum} {Suction} {Grasp} {Targets} in {Point} {Clouds} using a {New} {Analytic} {Model} and {Deep} {Learning}},
	shorttitle = {Dex-{Net} 3.0},
	url = {http://arxiv.org/abs/1709.06670},
	abstract = {Vacuum-based end effectors are widely used in industry and are often preferred over parallel-jaw and multifinger grippers due to their ability to lift objects with a single point of contact. Suction grasp planners often target planar surfaces on point clouds near the estimated centroid of an object. In this paper, we propose a compliant suction contact model that computes the quality of the seal between the suction cup and local target surface and a measure of the ability of the suction grasp to resist an external gravity wrench. To characterize grasps, we estimate robustness to perturbations in end-effector and object pose, material properties, and external wrenches. We analyze grasps across 1,500 3D object models to generate Dex-Net 3.0, a dataset of 2.8 million point clouds, suction grasps, and grasp robustness labels. We use Dex-Net 3.0 to train a Grasp Quality Convolutional Neural Network (GQ-CNN) to classify robust suction targets in point clouds containing a single object. We evaluate the resulting system in 350 physical trials on an ABB YuMi fitted with a pneumatic suction gripper. When evaluated on novel objects that we categorize as Basic (prismatic or cylindrical), Typical (more complex geometry), and Adversarial (with few available suction-grasp points) Dex-Net 3.0 achieves success rates of 98\${\textbackslash}\%\$, 82\${\textbackslash}\%\$, and 58\${\textbackslash}\%\$ respectively, improving to 81\${\textbackslash}\%\$ in the latter case when the training set includes only adversarial objects. Code, datasets, and supplemental material can be found at http://berkeleyautomation.github.io/dex-net .},
	urldate = {2021-04-20},
	journal = {arXiv:1709.06670 [cs]},
	author = {Mahler, Jeffrey and Matl, Matthew and Liu, Xinyu and Li, Albert and Gealy, David and Goldberg, Ken},
	month = apr,
	year = {2018},
	note = {arXiv: 1709.06670},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/T3EY2QKF/Mahler et al. - 2018 - Dex-Net 3.0 Computing Robust Robot Vacuum Suction.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/5LSEHEE9/1709.html:text/html},
}

@article{mahler_learning_2019,
	title = {Learning ambidextrous robot grasping policies},
	volume = {4},
	doi = {10.1126/scirobotics.aau4984},
	abstract = {Universal picking (UP), or reliable robot grasping of a diverse range of novel objects from heaps, is a grand challenge for e-commerce order fulfillment, manufacturing, inspection, and home service robots. Optimizing the rate, reliability, and range of UP is difficult due to inherent uncertainty in sensing, control, and contact physics. This paper explores “ambidextrous” robot grasping, where two or more heterogeneous grippers are used. We present Dexterity Network (Dex-Net) 4.0, a substantial extension to previous versions of Dex-Net that learns policies for a given set of grippers by training on synthetic datasets using domain randomization with analytic models of physics and geometry. We train policies for a parallel-jaw and a vacuum-based suction cup gripper on 5 million synthetic depth images, grasps, and rewards generated from heaps of three-dimensional objects. On a physical robot with two grippers, the Dex-Net 4.0 policy consistently clears bins of up to 25 novel objects with reliability greater than 95\% at a rate of more than 300 mean picks per hour.},
	journal = {Science Robotics},
	author = {Mahler, Jeffrey and Matl, Matthew and Satish, Vishal and Danielczuk, Michael and DeRose, Bill and McKinley, Stephen and Goldberg, Kenneth},
	month = jan,
	year = {2019},
	pages = {eaau4984},
	file = {Full Text:/home/andrej/Zotero/storage/7UL53HME/Mahler et al. - 2019 - Learning ambidextrous robot grasping policies.pdf:application/pdf},
}

@article{osa_algorithmic_2018,
	title = {An {Algorithmic} {Perspective} on {Imitation} {Learning}},
	volume = {7},
	doi = {10.1561/2300000053},
	abstract = {As robots and other intelligent agents move from simple environments and problems to more complex, unstructured settings, manually programming their behavior has become increasingly challenging and expensive. Often, it is easier for a teacher to demonstrate a desired behavior rather than attempt to manually engineer it. This process of learning from demonstrations, and the study of algorithms to do so, is called imitation learning. This work provides an introduction to imitation learning. It covers the underlying assumptions, approaches, and how they relate; the rich set of algorithms developed to tackle the problem; and advice on effective tools and implementation. We intend this paper to serve two audiences. First, we want to familiarize machine learning experts with the challenges of imitation learning, particularly those arising in robotics, and the interesting theoretical and practical distinctions between it and more familiar frameworks like statistical supervised learning theory and reinforcement learning. Second, we want to give roboticists and experts in applied artificial intelligence a broader appreciation for the frameworks and tools available for imitation learning.},
	journal = {Foundations and Trends in Robotics},
	author = {Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J. and Abbeel, Pieter and Peters, Jan},
	month = nov,
	year = {2018},
	pages = {1--179},
	file = {Full Text PDF:/home/andrej/Zotero/storage/648VP5IV/Osa et al. - 2018 - An Algorithmic Perspective on Imitation Learning.pdf:application/pdf},
}

@inproceedings{zhang_deep_2018,
	title = {Deep {Imitation} {Learning} for {Complex} {Manipulation} {Tasks} from {Virtual} {Reality} {Teleoperation}},
	doi = {10.1109/ICRA.2018.8461249},
	abstract = {Imitation learning is a powerful paradigm for robot skill acquisition. However, obtaining demonstrations suitable for learning a policy that maps from raw pixels to actions can be challenging. In this paper we describe how consumer-grade Virtual Reality headsets and hand tracking hardware can be used to naturally teleoperate robots to perform complex tasks. We also describe how imitation learning can learn deep neural network policies (mapping from pixels to actions) that can acquire the demonstrated skills. Our experiments showcase the effectiveness of our approach for learning visuomotor skills.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Chen, Xi and Goldberg, Ken and Abbeel, Pieter},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Grippers, Head, Neural networks, Robots, Task analysis, Three-dimensional displays, Visualization},
	pages = {5628--5635},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/JJV9P4ZJ/Zhang et al. - 2018 - Deep Imitation Learning for Complex Manipulation T.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/L56F4XHY/8461249.html:text/html},
}

@article{polydoros_survey_2017,
	title = {Survey of {Model}-{Based} {Reinforcement} {Learning}: {Applications} on {Robotics}},
	volume = {86},
	issn = {1573-0409},
	shorttitle = {Survey of {Model}-{Based} {Reinforcement} {Learning}},
	url = {https://doi.org/10.1007/s10846-017-0468-y},
	doi = {10.1007/s10846-017-0468-y},
	abstract = {Reinforcement learning is an appealing approach for allowing robots to learn new tasks. Relevant literature reveals a plethora of methods, but at the same time makes clear the lack of implementations for dealing with real life challenges. Current expectations raise the demand for adaptable robots. We argue that, by employing model-based reinforcement learning, the—now limited—adaptability characteristics of robotic systems can be expanded. Also, model-based reinforcement learning exhibits advantages that makes it more applicable to real life use-cases compared to model-free methods. Thus, in this survey, model-based methods that have been applied in robotics are covered. We categorize them based on the derivation of an optimal policy, the definition of the returns function, the type of the transition model and the learned task. Finally, we discuss the applicability of model-based reinforcement learning approaches in new applications, taking into consideration the state of the art in both algorithms and hardware.},
	language = {en},
	number = {2},
	urldate = {2021-04-23},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Polydoros, Athanasios S. and Nalpantidis, Lazaros},
	month = may,
	year = {2017},
	pages = {153--173},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/XP2ZRRIM/Polydoros and Nalpantidis - 2017 - Survey of Model-Based Reinforcement Learning Appl.pdf:application/pdf},
}

@inproceedings{deisenroth_pilco_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {{PILCO}: a model-based and data-efficient approach to policy search},
	isbn = {978-1-4503-0619-5},
	shorttitle = {{PILCO}},
	abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
	urldate = {2021-04-23},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	month = jun,
	year = {2011},
	pages = {465--472},
}

@incollection{durrant-whyte_learning_2012,
	title = {Learning to {Control} a {Low}-{Cost} {Manipulator} {Using} {Data}-{Efficient} {Reinforcement} {Learning}},
	isbn = {978-0-262-30596-9},
	url = {http://ieeexplore.ieee.org/document/6301026},
	abstract = {Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials—from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task.},
	urldate = {2021-04-23},
	booktitle = {Robotics: {Science} and {Systems} {VII}},
	publisher = {MIT Press},
	author = {Durrant-Whyte, Hugh and Roy, Nicholas and Abbeel, Pieter},
	year = {2012},
	note = {Conference Name: Robotics: Science and Systems VII},
	pages = {57--64},
	file = {IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/H6E84ZDV/6301026.html:text/html},
}

@inproceedings{quillen_deep_2018,
	title = {Deep {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Grasping}: {A} {Simulated} {Comparative} {Evaluation} of {Off}-{Policy} {Methods}},
	shorttitle = {Deep {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Grasping}},
	doi = {10.1109/ICRA.2018.8461039},
	abstract = {In this paper, we explore deep reinforcement learning algorithms for vision-based robotic grasping. Model-free deep reinforcement learning (RL) has been successfully applied to a range of challenging environments, but the proliferation of algorithms makes it difficult to discern which particular approach would be best suited for a rich, diverse task like grasping. To answer this question, we propose a simulated benchmark for robotic grasping that emphasizes off-policy learning and generalization to unseen objects. Off-policy learning enables utilization of grasping data over a wide variety of objects, and diversity is important to enable the method to generalize to new objects that were not seen during training. We evaluate the benchmark tasks against a variety of Q-function estimation methods, a method previously proposed for robotic grasping with deep neural network models, and a novel approach based on a combination of Monte Carlo return estimation and an off-policy correction. Our results indicate that several simple methods provide a surprisingly strong competitor to popular algorithms such as double Q-learning, and our analysis of stability sheds light on the relative tradeoffs between the algorithms1.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Quillen, Deirdre and Jang, Eric and Nachum, Ofir and Finn, Chelsea and Ibarz, Julian and Levine, Sergey},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Machine learning, Training, Robots, Task analysis, Benchmark testing, Grasping, Monte Carlo methods},
	pages = {6284--6291},
	file = {IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/BLZPWACV/8461039.html:text/html;Submitted Version:/home/andrej/Zotero/storage/5XVJVXCW/Quillen et al. - 2018 - Deep Reinforcement Learning for Vision-Based Robot.pdf:application/pdf},
}

@article{levine_learning_2016,
	title = {Learning {Hand}-{Eye} {Coordination} for {Robotic} {Grasping} with {Deep} {Learning} and {Large}-{Scale} {Data} {Collection}},
	volume = {37},
	doi = {10.1177/0278364917710318},
	abstract = {We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware. Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.},
	journal = {The International Journal of Robotics Research},
	author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
	month = mar,
	year = {2016},
	file = {Full Text:/home/andrej/Zotero/storage/QFVUI2ZT/Levine et al. - 2016 - Learning Hand-Eye Coordination for Robotic Graspin.pdf:application/pdf},
}

@article{breyer_comparing_2019,
	title = {Comparing {Task} {Simplifications} to {Learn} {Closed}-{Loop} {Object} {Picking} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {PP},
	doi = {10.1109/LRA.2019.2896467},
	abstract = {Enabling autonomous robots to interact in unstructured environments with dynamic objects requires manipulation capabilities that can deal with clutter, changes, and objects' variability. This paper presents a comparison of different reinforcement learning-based approaches for object picking with a robotic manipulator. We learn closed-loop policies mapping depth camera inputs to motion commands and compare different approaches to keep the problem tractable, including reward shaping, curriculum learning and using a policy pre-trained on a task with a reduced action set to warm- start the full problem. For efficient and more flexible data collection, we train in simulation and transfer the policies to a real robot. We show that using curriculum learning, policies learned with a sparse reward formulation can be trained at similar rates as with a shaped reward. These policies result in success rates comparable to the policy initialized on the simplified task. We could successfully transfer these policies to the real robot with only minor modifications of the depth image filtering. We found that using a heuristic to warm-start the training was useful to enforce desired behavior, while the policies trained from scratch using a curriculum learned better to cope with unseen scenarios where objects are removed.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Breyer, Michel and Furrer, Fadri and Novkovic, Tonci and Siegwart, Roland and Nieto, Juan},
	month = jan,
	year = {2019},
	pages = {1--1},
	file = {Submitted Version:/home/andrej/Zotero/storage/CP2A93X6/Breyer et al. - 2019 - Comparing Task Simplifications to Learn Closed-Loo.pdf:application/pdf},
}

@book{kim_acceleration_2020,
	title = {Acceleration of {Actor}-{Critic} {Deep} {Reinforcement} {Learning} for {Visual} {Grasping} in {Clutter} by {State} {Representation} {Learning} {Based} on {Disentanglement} of a {Raw} {Input} {Image}},
	abstract = {For a robotic grasping task in which diverse unseen target objects exist in a cluttered environment, some deep learning-based methods have achieved state-of-the-art results using visual input directly. In contrast, actor-critic deep reinforcement learning (RL) methods typically perform very poorly when grasping diverse objects, especially when learning from raw images and sparse rewards. To make these RL techniques feasible for vision-based grasping tasks, we employ state representation learning (SRL), where we encode essential information first for subsequent use in RL. However, typical representation learning procedures are unsuitable for extracting pertinent information for learning the grasping skill, because the visual inputs for representation learning, where a robot attempts to grasp a target object in clutter, are extremely complex. We found that preprocessing based on the disentanglement of a raw input image is the key to effectively capturing a compact representation. This enables deep RL to learn robotic grasping skills from highly varied and diverse visual inputs. We demonstrate the effectiveness of this approach with varying levels of disentanglement in a realistic simulated environment.},
	author = {Kim, Taewon and Park, Yeseong and Park, Youngbin and Suh, Il Hong},
	month = feb,
	year = {2020},
}

@inproceedings{joshi_robotic_2020,
	title = {Robotic {Grasping} using {Deep} {Reinforcement} {Learning}},
	doi = {10.1109/CASE48305.2020.9216986},
	abstract = {In this work, we present a deep reinforcement learning based method to solve the problem of robotic grasping using visio-motor feedback. The use of a deep learning based approach reduces the complexity caused by the use of hand-designed features. Our method uses an off-policy reinforcement learning framework to learn the grasping policy. We use the double deep Q-learning framework along with a novel GraspQ-Network to output grasp probabilities used to learn grasps that maximize the pick success. We propose a visual servoing mechanism that uses a multi-view camera setup that observes the scene which contains the objects of interest. We performed experiments using a Baxter Gazebo simulated environment as well as on the actual robot. The results show that our proposed method outperforms the baseline Q-learning framework and increases grasping accuracy by adapting a multi-view model in comparison to a single-view model.},
	booktitle = {2020 {IEEE} 16th {International} {Conference} on {Automation} {Science} and {Engineering} ({CASE})},
	author = {Joshi, Shirin and Kumra, Sulabh and Sahin, Ferat},
	month = aug,
	year = {2020},
	note = {ISSN: 2161-8089},
	keywords = {Grippers, Machine learning, Learning (artificial intelligence), Grasping, Cameras, Robot vision systems},
	pages = {1461--1466},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/7ATBJ497/Joshi et al. - 2020 - Robotic Grasping using Deep Reinforcement Learning.pdf:application/pdf},
}

@book{zhan_framework_2020,
	title = {A {Framework} for {Efficient} {Robotic} {Manipulation}},
	abstract = {Data-efficient learning of manipulation policies from visual observations is an outstanding challenge for real-robot learning. While deep reinforcement learning (RL) algorithms have shown success learning policies from visual observations, they still require an impractical number of real-world data samples to learn effective policies. However, recent advances in unsupervised representation learning and data augmentation significantly improved the sample efficiency of training RL policies on common simulated benchmarks. Building on these advances, we present a Framework for Efficient Robotic Manipulation (FERM) that utilizes data augmentation and unsupervised learning to achieve extremely sample-efficient training of robotic manipulation policies with sparse rewards. We show that, given only 10 demonstrations, a single robotic arm can learn sparse-reward manipulation policies from pixels, such as reaching, picking, moving, pulling a large object, flipping a switch, and opening a drawer in just 15-50 minutes of real-world training time. We include videos, code, and additional information on the project website -- https://sites.google.com/view/efficient-robotic-manipulation.},
	author = {Zhan, Albert and Zhao, Philip and Pinto, Lerrel and Abbeel, Pieter and Laskin, Michael},
	month = dec,
	year = {2020},
}

@inproceedings{haarnoja_composable_2018,
	title = {Composable {Deep} {Reinforcement} {Learning} for {Robotic} {Manipulation}},
	doi = {10.1109/ICRA.2018.8460756},
	abstract = {Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Machine learning, Training, Neural networks, Robots, Task analysis, Learning (artificial intelligence), Entropy},
	pages = {6244--6251},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/HQCBUJGA/Haarnoja et al. - 2018 - Composable Deep Reinforcement Learning for Robotic.pdf:application/pdf},
}

@inproceedings{gualtieri_pick_2018,
	title = {Pick and {Place} {Without} {Geometric} {Object} {Models}},
	doi = {10.1109/ICRA.2018.8460553},
	abstract = {We propose a novel formulation of robotic pick and place as a deep reinforcement learning (RL) problem. Whereas most deep RL approaches to robotic manipulation frame the problem in terms of low level states and actions, we propose a more abstract formulation. In this formulation, actions are target reach poses for the hand and states are a history of such reaches. We show this approach can solve a challenging class of pick-place and regrasping problems where the exact geometry of the objects to be handled is unknown. The only information our method requires is: 1) the sensor perception available to the robot at test time; 2) prior knowledge of the general class of objects for which the system was trained. We evaluate our method using objects belonging to two different categories, mugs and bottles, both in simulation and on real hardware. Results show a major improvement relative to a shape primitives baseline.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Gualtieri, Marcus and Pas, Andreas ten and Platt, Robert},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Task analysis, Three-dimensional displays, Geometry, History, Robot sensing systems, Shape},
	pages = {7433--7440},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/XIZ7KWXF/Gualtieri et al. - 2018 - Pick and Place Without Geometric Object Models.pdf:application/pdf},
}

@article{wu_generative_2020,
	title = {Generative {Attention} {Learning}: a “{GenerAL}” framework for high-performance multi-fingered grasping in clutter},
	volume = {44},
	issn = {1573-7527},
	shorttitle = {Generative {Attention} {Learning}},
	url = {https://doi.org/10.1007/s10514-020-09907-y},
	doi = {10.1007/s10514-020-09907-y},
	abstract = {Generative Attention Learning (GenerAL) is a framework for high-DOF multi-fingered grasping that is not only robust to dense clutter and novel objects but also effective with a variety of different parallel-jaw and multi-fingered robot hands. This framework introduces a novel attention mechanism that substantially improves the grasp success rate in clutter. Its generative nature allows the learning of full-DOF grasps with flexible end-effector positions and orientations, as well as all finger joint angles of the hand. Trained purely in simulation, this framework skillfully closes the sim-to-real gap. To close the visual sim-to-real gap, this framework uses a single depth image as input. To close the dynamics sim-to-real gap, this framework circumvents continuous motor control with a direct mapping from pixel to Cartesian space inferred from the same depth image. Finally, this framework demonstrates inter-robot generality by achieving over \$\$92{\textbackslash}\%\$\$real-world grasp success rates in cluttered scenes with novel objects using two multi-fingered robotic hand-arm systems with different degrees of freedom.},
	language = {en},
	number = {6},
	urldate = {2021-04-24},
	journal = {Autonomous Robots},
	author = {Wu, Bohan and Akinola, Iretiayo and Gupta, Abhi and Xu, Feng and Varley, Jacob and Watkins-Valls, David and Allen, Peter K.},
	month = jul,
	year = {2020},
	pages = {971--990},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/K6Y2TZIB/Wu et al. - 2020 - Generative Attention Learning a “GenerAL” framewo.pdf:application/pdf},
}

@inproceedings{liu_active_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Active {Affordance} {Exploration} for {Robot} {Grasping}},
	isbn = {978-3-030-27541-9},
	doi = {10.1007/978-3-030-27541-9_35},
	abstract = {Robotic grasp in complicated un-structured warehouse environments is still a challenging task and attracts lots of attentions from robot vision and machine learning communities. A popular strategy is to directly detect the graspable region for specific end-effector such as suction cup, two-fingered gripper or multi-fingered hand. However, those work usually depends on the accurate object detection and precise pose estimation. Very recently, affordance map which describes the action possibilities that an environment can offer, begins to be used for grasp tasks. But it often fails in cluttered environments and degrades the efficiency of warehouse automation. In this paper, we establish an active exploration framework for robot grasp and design a deep reinforcement learning method. To verify the effectiveness, we develop a new composite hand which combines the suction cup and fingers and the experimental validations on robotic grasp tasks show the advantages of the active exploration method. This novel method significantly improves the grasp efficiency of the warehouse manipulators.},
	language = {en},
	booktitle = {Intelligent {Robotics} and {Applications}},
	publisher = {Springer International Publishing},
	author = {Liu, Huaping and Yuan, Yuan and Deng, Yuhong and Guo, Xiaofeng and Wei, Yixuan and Lu, Kai and Fang, Bin and Guo, Di and Sun, Fuchun},
	editor = {Yu, Haibin and Liu, Jinguo and Liu, Lianqing and Ju, Zhaojie and Liu, Yuwang and Zhou, Dalin},
	year = {2019},
	keywords = {Active exploration, Affordance map, Robotic grasp},
	pages = {426--438},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/VR477GAA/Liu et al. - 2019 - Active Affordance Exploration for Robot Grasping.pdf:application/pdf},
}

@inproceedings{zeng_learning_2018,
	title = {Learning {Synergies} {Between} {Pushing} and {Grasping} with {Self}-{Supervised} {Deep} {Reinforcement} {Learning}},
	doi = {10.1109/IROS.2018.8593986},
	abstract = {Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch through model-free deep reinforcement learning. Our method involves training two fully convolutional networks that map from visual observations to actions: one infers the utility of pushes for a dense pixel-wise sampling of end-effector orientations and locations, while the other does the same for grasping. Both networks are trained jointly in a Q-learning framework and are entirely self-supervised by trial and error, where rewards are provided from successful grasps. In this way, our policy learns pushing motions that enable future grasps, while learning grasps that can leverage past pushes. During picking experiments in both simulation and real-world scenarios, we find that our system quickly learns complex behaviors even amid challenging cases of tightly packed clutter, and achieves better grasping success rates and picking efficiencies than baseline alternatives after a few hours of training. We further demonstrate that our method is capable of generalizing to novel objects. Qualitative results (videos), code, pre-trained models, and simulation environments are available at http://vpg.cs.princeton.edu/},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Zeng, Andy and Song, Shuran and Welker, Stefan and Lee, Johnny and Rodriguez, Alberto and Funkhouser, Thomas},
	month = oct,
	year = {2018},
	note = {ISSN: 2153-0866},
	keywords = {Training, Three-dimensional displays, Grasping, Manipulators, Planning, Reinforcement learning},
	pages = {4238--4245},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/R4MZFJV8/Zeng et al. - 2018 - Learning Synergies Between Pushing and Grasping wi.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/7PPDTIQ5/8593986.html:text/html},
}

@article{popov_data-efficient_2017,
	title = {Data-efficient {Deep} {Reinforcement} {Learning} for {Dexterous} {Manipulation}},
	url = {http://arxiv.org/abs/1704.03073},
	abstract = {Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.},
	urldate = {2021-04-24},
	journal = {arXiv:1704.03073 [cs]},
	author = {Popov, Ivaylo and Heess, Nicolas and Lillicrap, Timothy and Hafner, Roland and Barth-Maron, Gabriel and Vecerik, Matej and Lampe, Thomas and Tassa, Yuval and Erez, Tom and Riedmiller, Martin},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.03073},
	keywords = {Computer Science - Robotics, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/M8XTRSQ9/Popov et al. - 2017 - Data-efficient Deep Reinforcement Learning for Dex.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/GBCZYQWD/1704.html:text/html},
}

@inproceedings{tobin_domain_2017,
	title = {Domain randomization for transferring deep neural networks from simulation to the real world},
	doi = {10.1109/IROS.2017.8202133},
	abstract = {Bridging the `reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
	booktitle = {2017 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	month = sep,
	year = {2017},
	note = {ISSN: 2153-0866},
	keywords = {Training, Robots, Three-dimensional displays, Cameras, Adaptation models, Data models, Solid modeling},
	pages = {23--30},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/JRM7MRPZ/Tobin et al. - 2017 - Domain randomization for transferring deep neural .pdf:application/pdf},
}

@article{islam_reproducibility_2017,
	title = {Reproducibility of {Benchmarked} {Deep} {Reinforcement} {Learning} {Tasks} for {Continuous} {Control}},
	abstract = {Policy gradient methods in reinforcement learning have become increasingly prevalent for state-of-the-art performance in continuous control tasks. Novel methods typically benchmark against a few key algorithms such as deep deterministic policy gradients and trust region policy optimization. As such, it is important to present and use consistent baselines experiments. However, this can be difficult due to general variance in the algorithms, hyper-parameter tuning, and environment stochasticity. We investigate and discuss: the significance of hyper-parameters in policy gradients for continuous control, general variance in the algorithms, and reproducibility of reported results. We provide guidelines on reporting novel results as comparisons against baseline methods such that future researchers can make informed decisions when investigating novel methods.},
	author = {Islam, Riashat and Henderson, Peter and Gomrokchi, Maziar and Precup, Doina},
	month = aug,
	year = {2017},
	file = {Full Text PDF:/home/andrej/Zotero/storage/H6XYUW7U/Islam et al. - 2017 - Reproducibility of Benchmarked Deep Reinforcement .pdf:application/pdf},
}

@book{kalashnikov_qt-opt_2018,
	title = {{QT}-{Opt}: {Scalable} {Deep} {Reinforcement} {Learning} for {Vision}-{Based} {Robotic} {Manipulation}},
	shorttitle = {{QT}-{Opt}},
	abstract = {In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96\% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.},
	author = {Kalashnikov, Dmitry and Irpan, Alex and Pastor, Peter and Ibarz, Julian and Herzog, Alexander and Jang, Eric and Quillen, Deirdre and Holly, Ethan and Kalakrishnan, Mrinal and Vanhoucke, Vincent and Levine, Sergey},
	month = jun,
	year = {2018},
	file = {Full Text PDF:/home/andrej/Zotero/storage/5GWJ9TCP/Kalashnikov et al. - 2018 - QT-Opt Scalable Deep Reinforcement Learning for V.pdf:application/pdf},
}

@book{gualtieri_learning_2018,
	title = {Learning 6-{DoF} {Grasping} and {Pick}-{Place} {Using} {Attention} {Focus}},
	abstract = {We address a class of manipulation problems where the robot perceives the scene with a depth sensor and can move its end effector in a space with six degrees of freedom -- 3D position and orientation. Our approach is to formulate the problem as a Markov decision process (MDP) with abstract yet generally applicable state and action representations. Finding a good solution to the MDP requires adding constraints on the allowed actions. We develop a specific set of constraints called hierarchical SE(3) sampling (HSE3S) which causes the robot to learn a sequence of gazes to focus attention on the task-relevant parts of the scene. We demonstrate the effectiveness of our approach on three challenging pick-place tasks (with novel objects in clutter and nontrivial places) both in simulation and on a real robot, even though all training is done in simulation.},
	author = {Gualtieri, Marcus and Platt, Robert},
	month = jun,
	year = {2018},
}

@inproceedings{osa_experiments_2017,
	address = {Cham},
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {Experiments with {Hierarchical} {Reinforcement} {Learning} of {Multiple} {Grasping} {Policies}},
	isbn = {978-3-319-50115-4},
	doi = {10.1007/978-3-319-50115-4_15},
	abstract = {Robotic grasping has attracted considerable interest, but it still remains a challenging task. The data-driven approach is a promising solution to the robotic grasping problem; this approach leverages a grasp dataset and generalizes grasps for various objects. However, these methods often depend on the quality of the given datasets, which are not trivial to obtain with sufficient quality. Although reinforcement learning approaches have been recently used to achieve autonomous collection of grasp datasets, the existing algorithms are often limited to specific grasp types. In this paper, we present a framework for hierarchical reinforcement learning of grasping policies. In our framework, the lower-level hierarchy learns multiple grasp types, and the upper-level hierarchy learns a policy to select from the learned grasp types according to a point cloud of a new object. Through experiments, we validate that our approach learns grasping by constructing the grasp dataset autonomously. The experimental results show that our approach learns multiple grasping policies and generalizes the learned grasps by using local point cloud information.},
	language = {en},
	booktitle = {2016 {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Osa, Takayuki and Peters, Jan and Neumann, Gerhard},
	editor = {Kulić, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
	year = {2017},
	keywords = {Grasping, Reinforcement learning, Point clouds},
	pages = {160--172},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/KXVZZK7X/Osa et al. - 2017 - Experiments with Hierarchical Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{henderson_deep_2018,
	title = {Deep {Reinforcement} {Learning} that {Matters}},
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	booktitle = {{AAAI}},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, D.},
	year = {2018},
	file = {Full Text PDF:/home/andrej/Zotero/storage/JFTMHHCN/Henderson et al. - 2018 - Deep Reinforcement Learning that Matters.pdf:application/pdf},
}

@inproceedings{bousmalis_using_2018,
	title = {Using {Simulation} and {Domain} {Adaptation} to {Improve} {Efficiency} of {Deep} {Robotic} {Grasping}},
	url = {https://arxiv.org/abs/1709.07857},
	urldate = {2021-04-30},
	author = {Bousmalis, Konstantinos and Irpan, Alex and Wohlhart, Paul and Bai, Yunfei and Kelcey, Matthew and Kalakrishnan, Mrinal and Downs, Laura and Ibarz, Julian and Sampedro, Peter Pastor and Konolige, Kurt and Levine, Sergey and Vanhoucke, Vincent},
	year = {2018},
}

@article{zhang_towards_2015,
	title = {Towards {Vision}-{Based} {Deep} {Reinforcement} {Learning} for {Robotic} {Motion} {Control}},
	abstract = {This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only. The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time. We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation. A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation. Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.},
	author = {Zhang, Fangyi and Leitner, Juxi and Milford, Michael and Upcroft, Ben and Corke, Peter},
	month = nov,
	year = {2015},
	file = {Full Text PDF:/home/andrej/Zotero/storage/B4YEHQFI/Zhang et al. - 2015 - Towards Vision-Based Deep Reinforcement Learning f.pdf:application/pdf},
}

@inproceedings{iqbal_toward_2020,
	title = {Toward {Sim}-to-{Real} {Directional} {Semantic} {Grasping}},
	doi = {10.1109/ICRA40945.2020.9197310},
	abstract = {We address the problem of directional semantic grasping, that is, grasping a specific object from a specific direction. We approach the problem using deep reinforcement learning via a double deep Q-network (DDQN) that learns to map downsampled RGB input images from a wrist-mounted camera to Q-values, which are then translated into Cartesian robot control commands via the cross-entropy method (CEM). The network is learned entirely on simulated data generated by a custom robot simulator that models both physical reality (contacts) and perceptual quality (high-quality rendering). The reality gap is bridged using domain randomization. The system is an example of end-to-end (mapping input monocular RGB images to output Cartesian motor commands) grasping of objects from multiple pre-defined object-centric orientations, such as from the side or top. We show promising results in both simulation and the real world, along with some challenges faced and the need for future research in this area.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Iqbal, Shariq and Tremblay, Jonathan and Campbell, Andy and Leung, Kirby and To, Thang and Cheng, Jia and Leitch, Erik and McKay, Duncan and Birchfield, Stan},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Grippers, Training, Grasping, Cameras, Robot vision systems},
	pages = {7247--7253},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/BUVVCQUY/Iqbal et al. - 2020 - Toward Sim-to-Real Directional Semantic Grasping.pdf:application/pdf},
}

@article{laskin_reinforcement_2020,
	title = {Reinforcement {Learning} with {Augmented} {Data}},
	url = {http://arxiv.org/abs/2004.14990},
	abstract = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
	urldate = {2021-04-30},
	journal = {arXiv:2004.14990 [cs, stat]},
	author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.14990},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/andrej/Zotero/storage/ZSGGGC5J/Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf:application/pdf;arXiv.org Snapshot:/home/andrej/Zotero/storage/P7JNMAGV/2004.html:text/html},
}

@book{singh_end--end_2019,
	title = {End-to-{End} {Robotic} {Reinforcement} {Learning} without {Reward} {Engineering}},
	abstract = {The combination of deep neural network models and reinforcement learning algorithms can make it possible to learn policies for robotic behaviors that directly read in raw sensory inputs, such as camera images, effectively subsuming both estimation and control into one model. However, real-world applications of reinforcement learning must specify the goal of the task by means of a manually programmed reward function, which in practice requires either designing the very same perception pipeline that end-to-end reinforcement learning promises to avoid, or else instrumenting the environment with additional sensors to determine if the task has been performed successfully. In this paper, we propose an approach for removing the need for manual engineering of reward specifications by enabling a robot to learn from a modest number of examples of successful outcomes, followed by actively solicited queries, where the robot shows the user a state and asks for a label to determine whether that state represents successful completion of the task. While requesting labels for every single state would amount to asking the user to manually provide the reward signal, our method requires labels for only a tiny fraction of the states seen during training, making it an efficient and practical approach for learning skills without manually engineered rewards. We evaluate our method on real-world robotic manipulation tasks where the observations consist of images viewed by the robot's camera. In our experiments, our method effectively learns to arrange objects, place books, and drape cloth, directly from images and without any manually specified reward functions, and with only 1-4 hours of interaction with the real world.},
	author = {Singh, Avi and Yang, Larry and Hartikainen, Kristian and Finn, Chelsea and Levine, Sergey},
	month = apr,
	year = {2019},
}

@book{narvekar_curriculum_2020,
	title = {Curriculum {Learning} for {Reinforcement} {Learning} {Domains}: {A} {Framework} and {Survey}},
	shorttitle = {Curriculum {Learning} for {Reinforcement} {Learning} {Domains}},
	abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
	author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew and Stone, Peter},
	month = mar,
	year = {2020},
	file = {Full Text PDF:/home/andrej/Zotero/storage/YRXIWFHN/Narvekar et al. - 2020 - Curriculum Learning for Reinforcement Learning Dom.pdf:application/pdf},
}

@article{plappert_multi-goal_2018,
	title = {Multi-{Goal} {Reinforcement} {Learning}: {Challenging} {Robotics} {Environments} and {Request} for {Research}},
	shorttitle = {Multi-{Goal} {Reinforcement} {Learning}},
	abstract = {The purpose of this technical report is two-fold. First of all, it introduces a suite of challenging continuous control tasks (integrated with OpenAI Gym) based on currently existing robotics hardware. The tasks include pushing, sliding and pick \& place with a Fetch robotic arm as well as in-hand object manipulation with a Shadow Dexterous Hand. All tasks have sparse binary rewards and follow a Multi-Goal Reinforcement Learning (RL) framework in which an agent is told what to do using an additional input. The second part of the paper presents a set of concrete research ideas for improving RL algorithms, most of which are related to Multi-Goal RL and Hindsight Experience Replay.},
	author = {Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
	month = feb,
	year = {2018},
}

@article{wang_o-cnn_2017,
	title = {O-{CNN}: {Octree}-based {Convolutional} {Neural} {Networks} for {3D} {Shape} {Analysis}},
	volume = {36},
	shorttitle = {O-{CNN}},
	doi = {10.1145/3072959.3073608},
	abstract = {We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Peng-Shuai and Liu, Yang and Guo, Yu-Xiao and Sun, Chunyu and Tong, Xin},
	month = jul,
	year = {2017},
	pages = {1--11},
	file = {Full Text PDF:/home/andrej/Zotero/storage/CFUFYXN5/Wang et al. - 2017 - O-CNN Octree-based Convolutional Neural Networks .pdf:application/pdf},
}

@inproceedings{wang_adaptive_2018,
	title = {Adaptive {O}-{CNN}: a patch-based deep representation of {3D} shapes},
	volume = {37},
	shorttitle = {Adaptive {O}-{CNN}},
	doi = {10.1145/3272127.3275050},
	abstract = {We present an Adaptive Octree-based Convolutional Neural Network (Adaptive O-CNN) for efficient 3D shape encoding and decoding. Different from volumetric-based or octree-based CNN methods that represent a 3D shape with voxels in the same resolution, our method represents a 3D shape adaptively with octants at different levels and models the 3D shape within each octant with a planar patch. Based on this adaptive patch-based representation, we propose an Adaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The Adaptive O-CNN encoder takes the planar patch normal and displacement as input and performs 3D convolutions only at the octants at each level, while the Adaptive O-CNN decoder infers the shape occupancy and subdivision status of octants at each level and estimates the best plane normal and displacement for each leaf octant. As a general framework for 3D shape analysis and generation, the Adaptive O-CNN not only reduces the memory and computational cost, but also offers better shape generation capability than the existing 3D-CNN approaches. We validate Adaptive O-CNN in terms of efficiency and effectiveness on different shape analysis and generation tasks, including shape classification, 3D autoencoding, shape prediction from a single image, and shape completion for noisy and incomplete point clouds.},
	author = {Wang, Peng-Shuai and Sun, Chunyu and Liu, Yang and Tong, Xin},
	month = dec,
	year = {2018},
	pages = {1--11},
	file = {Submitted Version:/home/andrej/Zotero/storage/9AFPGMSM/Wang et al. - 2018 - Adaptive O-CNN a patch-based deep representation .pdf:application/pdf},
}

@book{ahmed_deep_2018,
	title = {Deep {Learning} {Advances} on {Different} {3D} {Data} {Representations}: {A} {Survey}},
	shorttitle = {Deep {Learning} {Advances} on {Different} {3D} {Data} {Representations}},
	abstract = {3D data is a valuable asset in the field of computer vision as it provides rich information about the full geometry of sensed objects and scenes. With the recent availability of large 3D datasets and the increase in computational power, it is today possible to consider applying deep learning to learn specific tasks on 3D data such as segmentation, recognition and correspondence. Depending on the considered 3D data representation, different challenges may be foreseen in using existent deep learning architectures. In this paper, we provide a comprehensive overview of various 3D data representations highlighting the difference between Euclidean and non-Euclidean ones. We also discuss how deep learning methods are applied on each representation, analyzing the challenges to overcome.},
	author = {Ahmed, Eman and Saint, Alexandre and Shabayek, Abdelrahman and Cherenkova, Kseniya and Das, Rig and Gusev, Gleb and Aouada, Djamila and Ottersten, Björn},
	month = aug,
	year = {2018},
	file = {Full Text PDF:/home/andrej/Zotero/storage/MPW2DM6Q/Ahmed et al. - 2018 - Deep Learning Advances on Different 3D Data Repres.pdf:application/pdf},
}

@inproceedings{wu_3d_2015,
	title = {{3D} {ShapeNets}: {A} deep representation for volumetric shapes},
	shorttitle = {{3D} {ShapeNets}},
	doi = {10.1109/CVPR.2015.7298801},
	abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Three-dimensional displays, Computational modeling, Shape, Planning, Solid modeling, Convolution, Object recognition},
	pages = {1912--1920},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/WL2YF39P/Wu et al. - 2015 - 3D ShapeNets A deep representation for volumetric.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/7JK8QZX8/7298801.html:text/html},
}

@inproceedings{maturana_voxnet_2015,
	title = {{VoxNet}: {A} {3D} {Convolutional} {Neural} {Network} for real-time object recognition},
	shorttitle = {{VoxNet}},
	doi = {10.1109/IROS.2015.7353481},
	abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Maturana, Daniel and Scherer, Sebastian},
	month = sep,
	year = {2015},
	keywords = {Feature extraction, Neural networks, Robots, Three-dimensional displays, Object recognition, Laser radar, Sensors},
	pages = {922--928},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/RAIR8PTZ/Maturana and Scherer - 2015 - VoxNet A 3D Convolutional Neural Network for real.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/6S8NQAYE/7353481.html:text/html},
}

@book{wang_deep_2020,
	title = {Deep {Octree}-based {CNNs} with {Output}-{Guided} {Skip} {Connections} for {3D} {Shape} and {Scene} {Completion}},
	abstract = {Acquiring complete and clean 3D shape and scene data is challenging due to geometric occlusion and insufficient views during 3D capturing. We present a simple yet effective deep learning approach for completing the input noisy and incomplete shapes or scenes. Our network is built upon the octree-based CNNs (O-CNN) with U-Net like structures, which enjoys high computational and memory efficiency and supports to construct a very deep network structure for 3D CNNs. A novel output-guided skip-connection is introduced to the network structure for better preserving the input geometry and learning geometry prior from data effectively. We show that with these simple adaptions -- output-guided skip-connection and deeper O-CNN (up to 70 layers), our network achieves state-of-the-art results in 3D shape completion and semantic scene computation.},
	author = {Wang, Peng-Shuai and Liu, Yang and Tong, Xin},
	month = jun,
	year = {2020},
}

@article{ruizhongtai_qi_pointnet_2016,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	author = {Ruizhongtai Qi, Charles and Su, Hao and Mo, Kaichun and Guibas, Leonidas},
	month = dec,
	year = {2016},
	file = {Full Text PDF:/home/andrej/Zotero/storage/6855P5HA/Ruizhongtai Qi et al. - 2016 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf},
}

@inproceedings{qi_pointnet_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {{PointNet}++: deep hierarchical feature learning on point sets in a metric space},
	isbn = {978-1-5108-6096-4},
	shorttitle = {{PointNet}++},
	abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
	urldate = {2021-05-06},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	month = dec,
	year = {2017},
	pages = {5105--5114},
	file = {Full Text PDF:/home/andrej/Zotero/storage/UCTPGDM4/Qi et al. - 2017 - PointNet++ deep hierarchical feature learning on .pdf:application/pdf},
}
