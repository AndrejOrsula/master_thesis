
@book{sutton_reinforcement_2018,
	address = {Cambridge, MA, USA},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	publisher = {A Bradford Book},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	pages = {529--533},
	file = {Snapshot:/home/andrej/Zotero/storage/KZVHNVLP/nature14236.html:text/html},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Submitted Version:/home/andrej/Zotero/storage/6IMRYRPU/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf},
}

@article{schrittwieser_mastering_2020,
	title = {Mastering {Atari}, {Go}, chess and shogi by planning with a learned model},
	volume = {588},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-03051-4},
	doi = {10.1038/s41586-020-03051-4},
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	language = {en},
	number = {7839},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	month = dec,
	year = {2020},
	note = {Number: 7839
Publisher: Nature Publishing Group},
	pages = {604--609},
	file = {Snapshot:/home/andrej/Zotero/storage/BE6MS6T2/s41586-020-03051-4.html:text/html;Submitted Version:/home/andrej/Zotero/storage/JATC7FIM/Schrittwieser et al. - 2020 - Mastering Atari, Go, chess and shogi by planning w.pdf:application/pdf},
}

@article{vinyals_grandmaster_2019,
	title = {Grandmaster level in {StarCraft} {II} using multi-agent reinforcement learning},
	volume = {575},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1724-z},
	doi = {10.1038/s41586-019-1724-z},
	abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players.},
	language = {en},
	number = {7782},
	urldate = {2021-04-12},
	journal = {Nature},
	author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Michaël and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, Rémi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and Wünsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
	month = nov,
	year = {2019},
	note = {Number: 7782
Publisher: Nature Publishing Group},
	pages = {350--354},
	file = {Snapshot:/home/andrej/Zotero/storage/5SDL7JC8/s41586-019-1724-z.html:text/html},
}

@article{sahbani_overview_2012,
	series = {Autonomous {Grasping}},
	title = {An overview of {3D} object grasp synthesis algorithms},
	volume = {60},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889011001485},
	doi = {10.1016/j.robot.2011.07.016},
	abstract = {This overview presents computational algorithms for generating 3D object grasps with autonomous multi-fingered robotic hands. Robotic grasping has been an active research subject for decades, and a great deal of effort has been spent on grasp synthesis algorithms. Existing papers focus on reviewing the mechanics of grasping and the finger–object contact interactions Bicchi and Kumar (2000) [12] or robot hand design and their control Al-Gallaf et al. (1993) [70]. Robot grasp synthesis algorithms have been reviewed in Shimoga (1996) [71], but since then an important progress has been made toward applying learning techniques to the grasping problem. This overview focuses on analytical as well as empirical grasp synthesis approaches.},
	language = {en},
	number = {3},
	urldate = {2021-04-18},
	journal = {Robotics and Autonomous Systems},
	author = {Sahbani, A. and El-Khoury, S. and Bidaud, P.},
	month = mar,
	year = {2012},
	keywords = {Force-closure, Grasp synthesis, Learning by demonstration, Task modeling},
	pages = {326--336},
	file = {ScienceDirect Full Text PDF:/home/andrej/Zotero/storage/9QSV9H4L/Sahbani et al. - 2012 - An overview of 3D object grasp synthesis algorithm.pdf:application/pdf;ScienceDirect Snapshot:/home/andrej/Zotero/storage/62GHM7TM/S0921889011001485.html:text/html},
}

@article{kroemer_review_2021,
	title = {A {Review} of {Robot} {Learning} for {Manipulation}: {Challenges}, {Representations}, and {Algorithms}},
	shorttitle = {A {Review} of {Robot} {Learning} for {Manipulation}},
	abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
	journal = {J. Mach. Learn. Res.},
	author = {Kroemer, Oliver and Niekum, S. and Konidaris, G.},
	year = {2021},
	file = {Full Text PDF:/home/andrej/Zotero/storage/SFIZLJYN/Kroemer et al. - 2021 - A Review of Robot Learning for Manipulation Chall.pdf:application/pdf},
}

@article{yun-hui_liu_complete_2004,
	title = {A complete and efficient algorithm for searching 3-{D} form-closure grasps in the discrete domain},
	volume = {20},
	issn = {1941-0468},
	doi = {10.1109/TRO.2004.829500},
	abstract = {A complete and efficient algorithm is proposed for searching form-closure grasps of n hard fingers on the surface of a three-dimensional object represented by discrete points. Both frictional and frictionless cases are considered. This algorithm starts to search a form-closure grasp from a randomly selected grasp using an efficient local search procedure until encountering a local minimum. The local search procedure employs the powerful ray-shooting technique to search in the direction of reducing the distance between the convex hull corresponding to the grasp and the origin of the wrench space. When the distance reaches a local minimum in the local search procedure, the algorithm decomposes the problem into a few subproblems in subsets of the points according to the existence conditions of form-closure grasps. A search tree whose root represents the original problem is employed to perform the searching process. The subproblems are represented as children of the root node and the same procedure is recursively applied to the children. It is proved that the search tree generates O(KlnK/n) nodes in case a from-closure grasp exists, where K is the number of the local minimum points of the distance in the grasp space and n is the number of fingers. Compared to the exhaustive search, this algorithm is more efficient, and, compared to other heuristic algorithms, the proposed algorithm is complete in the discrete domain. The efficiency of this algorithm is demonstrated by numerical examples.},
	number = {5},
	journal = {IEEE Transactions on Robotics},
	author = {{Yun-Hui Liu} and {Miu-Ling Lam} and Ding, D.},
	month = oct,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Automation, Computer science education, Councils, Educational programs, Educational technology, Fingers, Fixtures, Heuristic algorithms, Intelligent robots, Testing},
	pages = {805--816},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/B5VH23G6/Yun-Hui Liu et al. - 2004 - A complete and efficient algorithm for searching 3.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/BQRMWVGA/1339381.html:text/html},
}

@inproceedings{nguyen_constructing_1987,
	title = {Constructing stable grasps in {3D}},
	volume = {4},
	doi = {10.1109/ROBOT.1987.1088008},
	abstract = {This paper presents fast and simple algorithms for directly constructing stable grasps in 3D. The synthesis of stable grasps constructs virtual springs at the contacts, such that the grasped object is stable, and has a desired stiffness matrix about its stable equilibrium. The paper develops a simple geometric relation between the stiffness of the grasp and the spatial configuration of the virtual springs at the contacts. The stiffness of the grasp also depends on whether the points of contact stick, or slide without friction on the edges of the object.},
	booktitle = {1987 {IEEE} {International} {Conference} on {Robotics} and {Automation} {Proceedings}},
	author = {Nguyen, V.-},
	month = mar,
	year = {1987},
	keywords = {Artificial intelligence, Fingers, Friction, Paper technology, Research and development, Resists, Rubber, Springs, Stability, Tendons},
	pages = {234--239},
	file = {IEEE Xplore Full Text PDF:/home/andrej/Zotero/storage/6RUS2LUC/Nguyen - 1987 - Constructing stable grasps in 3D.pdf:application/pdf;IEEE Xplore Abstract Record:/home/andrej/Zotero/storage/A86EDLN3/1088008.html:text/html},
}

@article{roa_grasp_2015,
	title = {Grasp quality measures: review and performance},
	volume = {38},
	issn = {1573-7527},
	shorttitle = {Grasp quality measures},
	url = {https://doi.org/10.1007/s10514-014-9402-3},
	doi = {10.1007/s10514-014-9402-3},
	abstract = {The correct grasp of objects is a key aspect for the right fulfillment of a given task. Obtaining a good grasp requires algorithms to automatically determine proper contact points on the object as well as proper hand configurations, especially when dexterous manipulation is desired, and the quantification of a good grasp requires the definition of suitable grasp quality measures. This article reviews the quality measures proposed in the literature to evaluate grasp quality. The quality measures are classified into two groups according to the main aspect they evaluate: location of contact points on the object and hand configuration. The approaches that combine different measures from the two previous groups to obtain a global quality measure are also reviewed, as well as some measures related to human hand studies and grasp performance. Several examples are presented to illustrate and compare the performance of the reviewed measures.},
	language = {en},
	number = {1},
	urldate = {2021-04-19},
	journal = {Autonomous Robots},
	author = {Roa, Máximo A. and Suárez, Raúl},
	month = jan,
	year = {2015},
	pages = {65--88},
	file = {Springer Full Text PDF:/home/andrej/Zotero/storage/7X5NQZ6W/Roa and Suárez - 2015 - Grasp quality measures review and performance.pdf:application/pdf},
}
